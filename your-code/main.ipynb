{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-your-start:\" data-toc-modified-id=\"Before-your-start:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before your start:</a></span></li><li><span><a href=\"#Challenge-1---Explore-The-Dataset\" data-toc-modified-id=\"Challenge-1---Explore-The-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenge 1 - Explore The Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Explore-the-data-from-an-bird's-eye-view.\" data-toc-modified-id=\"Explore-the-data-from-an-bird's-eye-view.-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Explore the data from an bird's-eye view.</a></span></li><li><span><a href=\"#Next,-evaluate-if-the-columns-in-this-dataset-are-strongly-correlated.\" data-toc-modified-id=\"Next,-evaluate-if-the-columns-in-this-dataset-are-strongly-correlated.-2.0.0.2\"><span class=\"toc-item-num\">2.0.0.2&nbsp;&nbsp;</span>Next, evaluate if the columns in this dataset are strongly correlated.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-2---Remove-Column-Collinearity.\" data-toc-modified-id=\"Challenge-2---Remove-Column-Collinearity.-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Challenge 2 - Remove Column Collinearity.</a></span></li><li><span><a href=\"#Challenge-3---Handle-Missing-Values\" data-toc-modified-id=\"Challenge-3---Handle-Missing-Values-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenge 3 - Handle Missing Values</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#In-the-cells-below,-handle-the-missing-values-from-the-dataset.-Remember-to-comment-the-rationale-of-your-decisions.\" data-toc-modified-id=\"In-the-cells-below,-handle-the-missing-values-from-the-dataset.-Remember-to-comment-the-rationale-of-your-decisions.-4.0.0.1\"><span class=\"toc-item-num\">4.0.0.1&nbsp;&nbsp;</span>In the cells below, handle the missing values from the dataset. Remember to comment the rationale of your decisions.</a></span></li><li><span><a href=\"#Again,-examine-the-number-of-missing-values-in-each-column.\" data-toc-modified-id=\"Again,-examine-the-number-of-missing-values-in-each-column.-4.0.0.2\"><span class=\"toc-item-num\">4.0.0.2&nbsp;&nbsp;</span>Again, examine the number of missing values in each column.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-4---Handle-WHOIS_*-Categorical-Data\" data-toc-modified-id=\"Challenge-4---Handle-WHOIS_*-Categorical-Data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Challenge 4 - Handle <code>WHOIS_*</code> Categorical Data</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#In-the-cells-below,-fix-the-country-values-as-intructed-above.\" data-toc-modified-id=\"In-the-cells-below,-fix-the-country-values-as-intructed-above.-5.0.0.1\"><span class=\"toc-item-num\">5.0.0.1&nbsp;&nbsp;</span>In the cells below, fix the country values as intructed above.</a></span></li><li><span><a href=\"#If-a-limited-number-of-values-account-for-the-majority-of-data,-we-can-retain-these-top-values-and-re-label-all-other-rare-values.\" data-toc-modified-id=\"If-a-limited-number-of-values-account-for-the-majority-of-data,-we-can-retain-these-top-values-and-re-label-all-other-rare-values.-5.0.0.2\"><span class=\"toc-item-num\">5.0.0.2&nbsp;&nbsp;</span>If a limited number of values account for the majority of data, we can retain these top values and re-label all other rare values.</a></span></li><li><span><a href=\"#After-verifying,-now-let's-keep-the-top-10-values-of-the-column-and-re-label-other-columns-with-OTHER.\" data-toc-modified-id=\"After-verifying,-now-let's-keep-the-top-10-values-of-the-column-and-re-label-other-columns-with-OTHER.-5.0.0.3\"><span class=\"toc-item-num\">5.0.0.3&nbsp;&nbsp;</span>After verifying, now let's keep the top 10 values of the column and re-label other columns with <code>OTHER</code>.</a></span></li><li><span><a href=\"#In-the-next-cell,-drop-['WHOIS_STATEPRO',-'WHOIS_REGDATE',-'WHOIS_UPDATED_DATE'].\" data-toc-modified-id=\"In-the-next-cell,-drop-['WHOIS_STATEPRO',-'WHOIS_REGDATE',-'WHOIS_UPDATED_DATE'].-5.0.0.4\"><span class=\"toc-item-num\">5.0.0.4&nbsp;&nbsp;</span>In the next cell, drop <code>['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']</code>.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-5---Handle-Remaining-Categorical-Data-&amp;-Convert-to-Ordinal\" data-toc-modified-id=\"Challenge-5---Handle-Remaining-Categorical-Data-&amp;-Convert-to-Ordinal-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge 5 - Handle Remaining Categorical Data &amp; Convert to Ordinal</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#URL-is-easy.-We'll-simply-drop-it-because-it-has-too-many-unique-values-that-there's-no-way-for-us-to-consolidate.\" data-toc-modified-id=\"URL-is-easy.-We'll-simply-drop-it-because-it-has-too-many-unique-values-that-there's-no-way-for-us-to-consolidate.-6.0.0.1\"><span class=\"toc-item-num\">6.0.0.1&nbsp;&nbsp;</span><code>URL</code> is easy. We'll simply drop it because it has too many unique values that there's no way for us to consolidate.</a></span></li><li><span><a href=\"#Print-the-unique-value-counts-of-CHARSET.-You-see-there-are-only-a-few-unique-values.-So-we-can-keep-it-as-it-is.\" data-toc-modified-id=\"Print-the-unique-value-counts-of-CHARSET.-You-see-there-are-only-a-few-unique-values.-So-we-can-keep-it-as-it-is.-6.0.0.2\"><span class=\"toc-item-num\">6.0.0.2&nbsp;&nbsp;</span>Print the unique value counts of <code>CHARSET</code>. You see there are only a few unique values. So we can keep it as it is.</a></span></li><li><span><a href=\"#Before-you-think-of-your-own-solution,-don't-read-the-instructions-that-come-next.\" data-toc-modified-id=\"Before-you-think-of-your-own-solution,-don't-read-the-instructions-that-come-next.-6.0.0.3\"><span class=\"toc-item-num\">6.0.0.3&nbsp;&nbsp;</span>Before you think of your own solution, don't read the instructions that come next.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-6---Modeling,-Prediction,-and-Evaluation\" data-toc-modified-id=\"Challenge-6---Modeling,-Prediction,-and-Evaluation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Challenge 6 - Modeling, Prediction, and Evaluation</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#In-this-lab,-we-will-try-two-different-models-and-compare-our-results.\" data-toc-modified-id=\"In-this-lab,-we-will-try-two-different-models-and-compare-our-results.-7.0.0.1\"><span class=\"toc-item-num\">7.0.0.1&nbsp;&nbsp;</span>In this lab, we will try two different models and compare our results.</a></span></li><li><span><a href=\"#Our-second-algorithm-is-is-DecisionTreeClassifier\" data-toc-modified-id=\"Our-second-algorithm-is-is-DecisionTreeClassifier-7.0.0.2\"><span class=\"toc-item-num\">7.0.0.2&nbsp;&nbsp;</span>Our second algorithm is is DecisionTreeClassifier</a></span></li><li><span><a href=\"#We'll-create-another-DecisionTreeClassifier-model-with-max_depth=5.\" data-toc-modified-id=\"We'll-create-another-DecisionTreeClassifier-model-with-max_depth=5.-7.0.0.3\"><span class=\"toc-item-num\">7.0.0.3&nbsp;&nbsp;</span>We'll create another DecisionTreeClassifier model with max_depth=5.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Bonus-Challenge---Feature-Scaling\" data-toc-modified-id=\"Bonus-Challenge---Feature-Scaling-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Bonus Challenge - Feature Scaling</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will explore a dataset that describes websites with different features and labels them either benign or malicious . We will use supervised learning algorithms to figure out what feature patterns malicious websites are likely to have and use our model to predict malicious websites.\n",
    "\n",
    "Your features will be:\n",
    "\n",
    "+ URL: it is the anonymous identification of the URL analyzed in the study\n",
    "+ URL_LENGTH: it is the number of characters in the URL\n",
    "+ NUMBER_SPECIAL_CHARACTERS: it is number of special characters identified in the URL, such as, “/”, “%”, “#”, “&”, “. “, “=”\n",
    "+ CHARSET: it is a categorical value and its meaning is the character encoding standard (also called character set).\n",
    "+ SERVER: it is a categorical value and its meaning is the operative system of the server got from the packet response.\n",
    "+ CONTENT_LENGTH: it represents the content size of the HTTP header.\n",
    "+ WHOIS_COUNTRY: it is a categorical variable, its values are the countries we got from the server response (specifically, our script used the API of Whois).\n",
    "+ WHOIS_STATEPRO: it is a categorical variable, its values are the states we got from the server response (specifically, our script used the API of Whois).\n",
    "+ WHOIS_REGDATE: Whois provides the server registration date, so, this variable has date values with format DD/MM/YYY HH:MM\n",
    "+ WHOIS_UPDATED_DATE: Through the Whois we got the last update date from the server analyzed\n",
    "+ TCP_CONVERSATION_EXCHANGE: This variable is the number of TCP packets exchanged between the server and our honeypot client\n",
    "+ DIST_REMOTE_TCP_PORT: it is the number of the ports detected and different to TCP\n",
    "+ REMOTE_IPS: this variable has the total number of IPs connected to the honeypot\n",
    "+ APP_BYTES: this is the number of bytes transfered\n",
    "+ SOURCE_APP_PACKETS: packets sent from the honeypot to the server\n",
    "+ REMOTE_APP_PACKETS: packets received from the server\n",
    "+ APP_PACKETS: this is the total number of IP packets generated during the communication between the honeypot and the server\n",
    "+ DNS_QUERY_TIMES: this is the number of DNS packets generated during the communication between the honeypot and the server\n",
    "+ TYPE: this is a categorical variable, its values represent the type of web page analyzed, specifically, 1 is for malicious websites and 0 is for benign websites\n",
    "\n",
    "# Challenge 1 - Explore The Dataset\n",
    "\n",
    "Let's start by exploring the dataset. First load the data file:\n",
    "\n",
    "\n",
    "*Source: [kaggle](https://www.kaggle.com/viratkothari/malicious-and-benign-websites-classification)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = pd.read_csv('../website.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the data from an bird's-eye view.\n",
    "\n",
    "You should already been very familiar with the procedures now so we won't provide the instructions step by step. Reflect on what you did in the previous labs and explore the dataset.\n",
    "\n",
    "Things you'll be looking for:\n",
    "\n",
    "* What the dataset looks like?\n",
    "* What are the data types?\n",
    "* Which columns contain the features of the websites?\n",
    "* Which column contains the feature we will predict? What is the code standing for benign vs malicious websites?\n",
    "* Do we need to transform any of the columns from categorical to ordinal values? If so what are these columns?\n",
    "\n",
    "Feel free to add additional cells for your explorations. Make sure to comment what you find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the dataset looks like?\n",
    "print(f\"Dataset Shape: \\n{websites.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset Head: \\n{websites.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset Info: \\n{websites.info()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset Description: \\n{websites.describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values count per column\n",
    "print(websites.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the data types?\n",
    "print(websites.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which columns contain the features of the websites?\n",
    "# List columns except the target column\n",
    "features = websites.drop(columns=['Type']).columns\n",
    "print(\"Features:\", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which column contains the feature we will predict? What is the code standing for benign vs malicious websites?\n",
    "target_feature='Type'\n",
    "print(f\"The column with the feature to be predicted is: {target_feature}\")\n",
    "print(\"Unique values in target column:\", websites['Type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 for bening 1 for maliciuos websites\n",
    "\n",
    "#New column with the description\n",
    "#websites['Type_Description'] = websites['Type'].map({0: 'benign', 1: 'malicious'})\n",
    "#print(\"Unique values for column named 'Type_Description':\", websites['Type_Description'].unique())\n",
    "\n",
    "# Mapping target values to their meanings\n",
    "target_values = {0: \"benign\", 1: \"malicious\"}\n",
    "\n",
    "# Print each target value with its meaning\n",
    "for value, meaning in target_values.items():\n",
    "    print(f\"The value {value} stands for {meaning} websites.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we need to transform any of the columns from categorical to ordinal values? If so what are these columns?\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = websites.select_dtypes(include=['object', 'category']).columns\n",
    "print(\"Categorical columns:\", categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique values in each categorical column\n",
    "for column in categorical_columns:\n",
    "    print(f\"Unique values in {column}: {websites[column].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Columns Cardinality Check\n",
    "for column in categorical_columns:\n",
    "    print(f\"Cardinality of {column}: {websites[column].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to understand the relationship between WHOIS_STATEPRO and Type and make a decision of using or not this column\n",
    "\n",
    "# Table of Crossed frequency between WHOIS_STATEPRO and Type\n",
    "print(websites.groupby(['WHOIS_STATEPRO', 'Type']).size().reset_index(name='Count'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create a contingency table between WHOIS_STATEPRO and Type\n",
    "contingency_table = pd.crosstab(websites['WHOIS_STATEPRO'], websites['Type'])\n",
    "\n",
    "# Apply Chi-squared test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Show results\n",
    "print(f\"Chi-squared statistic: {chi2}\")\n",
    "print(f\"P-value: {p}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    print(\"Column WHOIS_STATEPRO is related with the 'Type' variable.\")\n",
    "else:\n",
    "    print(\"No enough evidence to confirm that WHOIS_STATEPRO is related to Type.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Columns Evaluation Summary\n",
    "''' \n",
    "Null values to manage:\n",
    "        1) CONTENT_LENGTH; fill null values with data median result\n",
    "            websites['CONTENT_LENGTH'].fillna(websites['CONTENT_LENGTH'].median(), inplace=True)\n",
    "            or remove it using \n",
    "            websites.drop(columns=['CONTENT_LENGTH'], inplace=True)\n",
    "        2) WHOIS_COUNTRY; fill null values with \"Unknown\"\n",
    "                websites['WHOIS_COUNTRY'].fillna(\"Unknown\", inplace=True)\n",
    "        3) WHOIS_STATEPRO; fill null values with \"Unknown\"\n",
    "                websites['WHOIS_STATEPRO'].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "Dates Convert into date format and extract info:\n",
    "        1) WHOIS_REGDATE;\n",
    "                websites['WHOIS_REGDATE'] = pd.to_datetime(websites['WHOIS_REGDATE'], errors='coerce')\n",
    "                websites['REG_YEAR'] = websites['WHOIS_REGDATE'].dt.year\n",
    "                websites['REG_MONTH'] = websites['WHOIS_REGDATE'].dt.month     \n",
    "        2) WHOIS_UPDATED_DATE\n",
    "                websites['WHOIS_UPDATED_DATE'] = pd.to_datetime(websites['WHOIS_UPDATED_DATE'], errors='coerce')\n",
    "                websites['UPDATED_YEAR'] = websites['WHOIS_UPDATED_DATE'].dt.year\n",
    "                websites['UPDATED_MONTH'] = websites['WHOIS_UPDATED_DATE'].dt.month\n",
    "\n",
    "Transformation:\n",
    "        1) Use of One-hot encoding for Low Cardinality\n",
    "                websites = pd.get_dummies(websites, columns=['CHARSET', 'WHOIS_COUNTRY'], drop_first=True)\n",
    "        2) Use of Label encoding for High Cardinality\n",
    "                websites['SERVER'] = websites['SERVER'].astype('category').cat.codes\n",
    "                websites['WHOIS_STATEPRO'] = websites['WHOIS_STATEPRO'].astype('category').cat.codes\n",
    "\n",
    "Column not to be used:\n",
    "        1) URL\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, evaluate if the columns in this dataset are strongly correlated.\n",
    "\n",
    "In the Mushroom supervised learning lab we did recently, we mentioned we are concerned if our dataset has strongly correlated columns because if it is the case we need to choose certain ML algorithms instead of others. We need to evaluate this for our dataset now.\n",
    "\n",
    "Luckily, most of the columns in this dataset are ordinal which makes things a lot easier for us. In the next cells below, evaluate the level of collinearity of the data.\n",
    "\n",
    "We provide some general directions for you to consult in order to complete this step:\n",
    "\n",
    "1. You will create a correlation matrix using the numeric columns in the dataset.\n",
    "\n",
    "1. Create a heatmap using `seaborn` to visualize which columns have high collinearity.\n",
    "\n",
    "1. Comment on which columns you might need to remove due to high collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "num_columns = websites.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Create the Correlation Matrix\n",
    "c_mx = websites[num_columns].corr()\n",
    "\n",
    "# Display Matrix\n",
    "print(\"Correlation Matrix:\")\n",
    "print(c_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap Creation\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(c_mx, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collinearity Evaluation Summary:\n",
    "'''\n",
    "1.00 => REMOTE_APP_BYTES    vs  APP_BYTES\n",
    "1.00 => APP_PACKETS         vs  TCP_CONVERSATION_EXCHANGE\n",
    "1.00 => SOURCE_APP_PACKETS  vs  APP_PACKETS\n",
    "1.00 => SOURCE_APP_PACKETS  vs  TCP_CONVERSATION_EXCHANGE\n",
    "0.99 => SOURCE_APP_PACKETS  vs  REMOTE_APP_PACKETS  \n",
    "0.99 => APP_PACKETS         vs  REMOTE_APP_PACKETS\n",
    "0.99 => REMOTE_APP_PACKETS  vs  TCP_CONVERSATION_EXCHANGE\n",
    "0.92 => URL_LENGTH          vs  NUMBER_SPECIAL_CHARACTERS\n",
    "0.88 => SOURCE_APP_BYTES    vs  REMOTE_APP_PACKETS\n",
    "0.87 => SOURCE_APP_BYTES    vs  TCP_CONVERSATION_EXCHANGE\n",
    "0.86 => SOURCE_APP_PACKETS  vs  SOURCE_APP_BYTES\n",
    "0.86 => SOURCE_APP_BYTES    vs  APP_PACKETS\n",
    "\n",
    "Columns To be Removed:\n",
    "1) REMOTE_APP_BYTES\n",
    "2) TCP_CONVERSATION_EXCHANGE\n",
    "3) SOURCE_APP_PACKETS\n",
    "4) REMOTE_APP_PACKETS\n",
    "\n",
    "# Removal Redudndant and high Collinerity Columns\n",
    "    websites.drop(columns=['REMOTE_APP_BYTES', 'TCP_CONVERSATION_EXCHANGE', 'REMOTE_APP_PACKETS', 'SOURCE_APP_PACKETS'], inplace=True)\n",
    "\n",
    "To evaluate if maintain or create new column to display a ratio between URL_LENGTH and NUMBER_SPECIAL_CHARACTERS due to their collinearity of 0.92:\n",
    "    websites['URL_CHAR_RATIO'] = websites['NUMBER_SPECIAL_CHARACTERS'] / websites['URL_LENGTH']\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is an example to know the feacture importance using a ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install xgboost\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade xgboost\n",
    "#!pip install --upgrade scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XGBRFClassifier model parameters\n",
    "params = {\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bynode': 0.8,\n",
    "    'max_depth': 3,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Instantiate XGBRFClassifier with the parameters\n",
    "model = xgb.XGBRFClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Define XGBClassifier model parameters\n",
    "params = {\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bynode': 0.8,\n",
    "    'max_depth': 3,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Instantiate XGBClassifier with the parameters\n",
    "model = xgb.XGBClassifier(**params)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define features and target\n",
    "X = websites._get_numeric_data().drop('Type', axis=1)\n",
    "y = websites['Type']\n",
    "\n",
    "# Create and fit the XGBoost classifier\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')  # Add `eval_metric` to avoid warnings\n",
    "xgb.fit(X, y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = websites._get_numeric_data().drop('Type', axis=1)\n",
    "y = websites.Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn; print(sklearn.__version__)\n",
    "#import xgboost; print(xgboost.__version__)\n",
    "#!pip install scikit-learn==1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To obtain the importance of each characteristic\n",
    "feature_importance = model.get_booster().get_score(importance_type='weight')\n",
    "\n",
    "# To convert into ordered format\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# To display the ordered characteristics\n",
    "feature_names, importance_values = zip(*sorted_features)\n",
    "\n",
    "# To create a bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, importance_values)\n",
    "plt.xlabel(\"Feature Importance (Weight)\")\n",
    "plt.title(\"Feature Importance from XGBRFClassifier\")\n",
    "plt.gca().invert_yaxis()  # Invierte el eje para mostrar la más importante arriba\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort_idx = xgb.feature_importances_.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.barh(X.columns[sort_idx],xgb.feature_importances_[sort_idx])\n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In the previous plot we can see the feactures with lower weight in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Remove Column Collinearity.\n",
    "\n",
    "From the heatmap you created, you should have seen at least 3 columns that can be removed due to high collinearity. Remove these columns from the dataset.\n",
    "\n",
    "Note that you should remove as few columns as you can. You don't have to remove all the columns at once. But instead, try removing one column, then produce the heatmap again to determine if additional columns should be removed. As long as the dataset no longer contains columns that are correlated for over 90%, you can stop. Also, keep in mind when two columns have high collinearity, you only need to remove one of them but not both.\n",
    "\n",
    "In the cells below, remove as few columns as you can to eliminate the high collinearity in the dataset. Make sure to comment on your way so that the instructional team can learn about your thinking process which allows them to give feedback. At the end, print the heatmap again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "#Removal of 1.00 Collinerity Columns\n",
    "websites.drop(columns=['REMOTE_APP_BYTES', 'TCP_CONVERSATION_EXCHANGE', 'SOURCE_APP_PACKETS'], inplace=True)\n",
    "\n",
    "num_columns = websites.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Create the Correlation Matrix\n",
    "c_mx = websites[num_columns].corr()\n",
    "\n",
    "# Display Matrix\n",
    "print(\"Correlation Matrix:\")\n",
    "print(c_mx)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(c_mx, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal of 0.99 Collinerity Columns\n",
    "websites.drop(columns=['REMOTE_APP_PACKETS'], inplace=True)\n",
    "\n",
    "num_columns = websites.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Create the Correlation Matrix\n",
    "c_mx = websites[num_columns].corr()\n",
    "\n",
    "# Display Matrix\n",
    "print(\"Correlation Matrix:\")\n",
    "print(c_mx)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(c_mx, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE THE 4 COLUMNS WITH MORE COLLINEARITY\n",
    "'''\n",
    "Removed Columns:\n",
    "1) REMOTE_APP_BYTES\n",
    "2) TCP_CONVERSATION_EXCHANGE\n",
    "3) SOURCE_APP_PACKETS\n",
    "4) REMOTE_APP_PACKETS\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Handle Missing Values\n",
    "\n",
    "The next step would be handling missing values. **We start by examining the number of missing values in each column, which you will do in the next cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Null values count per column\n",
    "print(websites.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firts we will drop the columns with more than 50% of missing data\n",
    "\n",
    "# Calculate null values and its Percentage (%)\n",
    "null_count = websites.isnull().sum()  # null values count\n",
    "total_rows = len(websites)  # Total of rows\n",
    "null_percentage = (null_count / total_rows) * 100  # Calculate null Percentage (%)\n",
    "\n",
    "# DataFrame for results\n",
    "null_summary = pd.DataFrame({\n",
    "    'Missing Values': null_count,\n",
    "    'Percentage (%)': null_percentage\n",
    "})\n",
    "\n",
    "# Show only columns with null values\n",
    "null_summary = null_summary[null_summary['Missing Values'] > 0]\n",
    "\n",
    "# Print Summary\n",
    "print(\"Null Values Summary:\")\n",
    "print(null_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will drop the rows with missing data\n",
    "print(websites.shape)\n",
    "\n",
    "columns_to_drop = null_percentage[null_percentage > 50].index \n",
    "websites = websites.drop(columns=columns_to_drop)\n",
    "\n",
    "print(websites.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, examine the number of missing values in each column. \n",
    "\n",
    "    If all cleaned, proceed. Otherwise, go back and do more cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine missing values in each column\n",
    "\n",
    "# No columns to be removed since non were more than 50% of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values count per column\n",
    "print(websites.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONTENT_LENGTH; fill null values with data median result\n",
    "websites['CONTENT_LENGTH'].fillna(websites['CONTENT_LENGTH'].median(), inplace=True)\n",
    "\n",
    "#DNS_QUERY_TIMES; fill null values with 0\n",
    "websites['DNS_QUERY_TIMES'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values count per column\n",
    "print(websites.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Handle `WHOIS_*` Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several categorical columns we need to handle. These columns are:\n",
    "\n",
    "* `URL`\n",
    "* `CHARSET`\n",
    "* `SERVER`\n",
    "* `WHOIS_COUNTRY`\n",
    "* `WHOIS_STATEPRO`\n",
    "* `WHOIS_REGDATE`\n",
    "* `WHOIS_UPDATED_DATE`\n",
    "\n",
    "How to handle string columns is always case by case. Let's start by working on `WHOIS_COUNTRY`. Your steps are:\n",
    "\n",
    "1. List out the unique values of `WHOIS_COUNTRY`.\n",
    "1. Consolidate the country values with consistent country codes. For example, the following values refer to the same country and should use consistent country code:\n",
    "    * `CY` and `Cyprus`\n",
    "    * `US` and `us`\n",
    "    * `SE` and `se`\n",
    "    * `GB`, `United Kingdom`, and `[u'GB'; u'UK']`\n",
    "\n",
    "#### In the cells below, fix the country values as intructed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique values in WHOIS_COUNTRY column\n",
    "unique_values = websites['WHOIS_COUNTRY'].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "websites.WHOIS_COUNTRY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Replace NaN with 'Unknown' in WHOIS_COUNTRY column\n",
    "websites['WHOIS_COUNTRY'].fillna('Unknown', inplace=True)\n",
    "\n",
    "good_country = {'None':'None', \n",
    "                'US':'US', \n",
    "                'SC':'SC', \n",
    "                'GB':'UK', \n",
    "                'UK':'UK', \n",
    "                'RU':'RU', \n",
    "                'AU':'AU', \n",
    "                'CA':'CA',\n",
    "                'PA':'PA',\n",
    "                'se':'SE', \n",
    "                'IN':'IN',\n",
    "                'LU':'LU', \n",
    "                'TH':'TH', \n",
    "                \"[u'GB'; u'UK']\":'UK', \n",
    "                'FR':'FR',\n",
    "                'NL':'NL',\n",
    "                'UG':'UG', \n",
    "                'JP':'JP', \n",
    "                'CN':'CN', \n",
    "                'SE':'SE',\n",
    "                'SI':'SI', \n",
    "                'IL':'IL', \n",
    "                'ru':'RU', \n",
    "                'KY':'KY', \n",
    "                'AT':'AT', \n",
    "                'CZ':'CZ', \n",
    "                'PH':'PH', \n",
    "                'BE':'BE', \n",
    "                'NO':'NO', \n",
    "                'TR':'TR', \n",
    "                'LV':'LV',\n",
    "                'DE':'DE', \n",
    "                'ES':'ES', \n",
    "                'BR':'BR', \n",
    "                'us':'US', \n",
    "                'KR':'KR', \n",
    "                'HK':'HK', \n",
    "                'UA':'UA', \n",
    "                'CH':'CH', \n",
    "                'United Kingdom':'UK',\n",
    "                'BS':'BS', \n",
    "                'PK':'PK', \n",
    "                'IT':'IT', \n",
    "                'Cyprus':'CY', \n",
    "                'BY':'BY', \n",
    "                'AE':'AE', \n",
    "                'IE':'IE', \n",
    "                'UY':'UY', \n",
    "                'KG':'KG'}\n",
    "\n",
    "#websites.WHOIS_COUNTRY = websites.WHOIS_COUNTRY.apply(lambda x : good_country[x])\n",
    "\n",
    "websites['WHOIS_COUNTRY'] = websites['WHOIS_COUNTRY'].apply(lambda x: good_country.get(x, 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.WHOIS_COUNTRY.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have fixed the country values, can we convert this column to ordinal now?\n",
    "\n",
    "Not yet. If you reflect on the previous labs how we handle categorical columns, you probably remember we ended up dropping a lot of those columns because there are too many unique values. Too many unique values in a column is not desirable in machine learning because it makes prediction inaccurate. But there are workarounds under certain conditions. One of the fixable conditions is:\n",
    "\n",
    "#### If a limited number of values account for the majority of data, we can retain these top values and re-label all other rare values.\n",
    "\n",
    "The `WHOIS_COUNTRY` column happens to be this case. You can verify it by print a bar chart of the `value_counts` in the next cell to verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bar_plot(x,y):\n",
    "    plt.bar(x, y)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_bar_plot(websites.WHOIS_COUNTRY.unique(),websites.WHOIS_COUNTRY.value_counts());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After verifying, now let's keep the top 10 values of the column and re-label other columns with `OTHER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Calculate the value counts for the WHOIS_COUNTRY column\n",
    "country_counts = websites['WHOIS_COUNTRY'].value_counts()\n",
    "\n",
    "# Identify the top 10 most frequent values\n",
    "top_10_countries = country_counts.index[:10]\n",
    "\n",
    "# Re-label less frequent values as 'OTHER'\n",
    "websites['WHOIS_COUNTRY'] = websites['WHOIS_COUNTRY'].apply(lambda x: x if x in top_10_countries else 'OTHER')\n",
    "\n",
    "# Display the updated value counts\n",
    "updated_country_counts = websites['WHOIS_COUNTRY'].value_counts()\n",
    "print(\"Updated value counts for WHOIS_COUNTRY column:\")\n",
    "print(updated_country_counts)\n",
    "\n",
    "# Plot the updated bar chart\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "updated_country_counts.plot(kind='bar')\n",
    "plt.title('Updated Value Counts of WHOIS_COUNTRY Column')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since `WHOIS_COUNTRY` has been re-labelled, we don't need `WHOIS_STATEPRO` any more because the values of the states or provinces may not be relevant any more. We'll drop this column.\n",
    "\n",
    "In addition, we will also drop `WHOIS_REGDATE` and `WHOIS_UPDATED_DATE`. These are the registration and update dates of the website domains. Not of our concerns.\n",
    "\n",
    "#### In the next cell, drop `['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites.drop(columns=['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(websites.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Handle Remaining Categorical Data & Convert to Ordinal\n",
    "\n",
    "Now print the `dtypes` of the data again. Besides `WHOIS_COUNTRY` which we already fixed, there should be 3 categorical columns left: `URL`, `CHARSET`, and `SERVER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "print(websites.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `URL` is easy. We'll simply drop it because it has too many unique values that there's no way for us to consolidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites.drop(columns=['URL'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the unique value counts of `CHARSET`. You see there are only a few unique values. So we can keep it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites.CHARSET.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SERVER` is a little more complicated. Print its unique values and think about how you can consolidate those values.\n",
    "\n",
    "#### Before you think of your own solution, don't read the instructions that come next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "websites.SERVER.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count `SERVER` value counts here\n",
    "\n",
    "server_counts = websites['SERVER'].value_counts()\n",
    "\n",
    "print(\"Unique value counts for the SERVER column:\")\n",
    "print(server_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are so many unique values in the `SERVER` column, there are actually only 3 main server types: `Microsoft`, `Apache`, and `nginx`. Just check if each `SERVER` value contains any of those server types and re-label them. For `SERVER` values that don't contain any of those substrings, label with `Other`.\n",
    "\n",
    "At the end, your `SERVER` column should only contain 4 unique values: `Microsoft`, `Apache`, `nginx`, and `Other`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-label function to update SERVER values\n",
    "def relabel_server(server):\n",
    "    if pd.isnull(server):  # Manejar valores nulos\n",
    "        return 'Other'\n",
    "    elif 'Microsoft' in server:\n",
    "        return 'Microsoft'\n",
    "    elif 'Apache' in server:\n",
    "        return 'Apache'\n",
    "    elif 'nginx' in server.lower():\n",
    "        return 'nginx'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply function to SERVER columns\n",
    "websites['SERVER'] = websites['SERVER'].apply(relabel_server)\n",
    "\n",
    "# Confirm unique values of SERVER\n",
    "print(\"Unique values after re-label:\")\n",
    "print(websites['SERVER'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_counts = websites['SERVER'].value_counts()\n",
    "\n",
    "print(\"Unique value counts for the SERVER column:\")\n",
    "print(server_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, all our categorical data are fixed now. **Let's convert them to ordinal data using Pandas' `get_dummies` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)).** Make sure you drop the categorical columns by passing `drop_first=True` to `get_dummies` as we don't need them any more. **Also, assign the data with dummy values to a new variable `website_dummy`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "website_dummy = pd.get_dummies(websites, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, inspect `website_dummy` to make sure the data and types are intended - there shouldn't be any categorical columns at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "website_dummy.head()\n",
    "website_dummy.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 6 - Modeling, Prediction, and Evaluation\n",
    "\n",
    "We'll start off this section by splitting the data to train and test. **Name your 4 variables `X_train`, `X_test`, `y_train`, and `y_test`. Select 80% of the data for training and 20% for testing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "# Assuming your DataFrame with dummy variables is named website_dummy\n",
    "# Define features (X) and target (y) variables\n",
    "X = website_dummy.drop('Type', axis=1)  # Replace 'target_column_name' with the actual target column name\n",
    "y = website_dummy['Type']  # Replace 'target_column_name' with the actual target column name\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the split data\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this lab, we will try two different models and compare our results.\n",
    "\n",
    "The first model we will use in this lab is logistic regression. We have previously learned about logistic regression as a classification algorithm. In the cell below, load `LogisticRegression` from scikit-learn and initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "# Display the model parameters\n",
    "print(\"Initialized Logistic Regression model:\")\n",
    "print(log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, fit the model to our training data. We have already separated our data into 4 parts. Use those in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler() \n",
    "X_train_scaled = scaler.fit_transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "log_reg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, import `confusion_matrix` and `accuracy_score` from `sklearn.metrics` and fit our testing data. Assign the fitted data to `y_pred` and print the confusion matrix as well as the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Importar libraries\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix and accuracy score\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are your thoughts on the performance of the model? Write your conclusions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your conclusions here:\n",
    "Model Performance Insights\n",
    "\n",
    "Strengths:\n",
    "\n",
    "The model performs very well on the majority class (class 0) with a low number of false positives.\n",
    "The overall accuracy is high, indicating the model generalizes well for the testing data.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "The model has a relatively high number of false negatives (30). This means it often misclassifies true class 1 samples as class 0.\n",
    "If class 1 is critical (e.g., identifying malicious websites), the model may need improvement to reduce false negatives.\n",
    "   \n",
    "The model achieves a good accuracy of 91.31%, with strong performance for class 0 but room for improvement in handling class 1. Further analysis with precision, recall, and strategies to handle false negatives could help improve the model’s robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our second algorithm is is DecisionTreeClassifier\n",
    "\n",
    "Though is it not required, we will fit a model using the training data and then test the performance of the model using the testing data. Start by loading `DecisionTreeClassifier` from scikit-learn and then initializing and fitting the model. We'll start off with a model where max_depth=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Initialize the Decision Tree Classifier with max_depth=3\n",
    "tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "tree_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Display the model parameters after fitting\n",
    "print(\"Decision Tree Classifier model fitted to training data:\")\n",
    "print(tree_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your model, compute the predicted probabilities, decide 0 or 1 using a threshold of 0.5 and print the confusion matrix as well as the accuracy score (on the test set!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Obtain the predicted probabilities\n",
    "probs = tree_clf.predict_proba(X_test)\n",
    "\n",
    "# Apply a threshold of 0.5 for classification\n",
    "threshold = 0.5\n",
    "y_pred_threshold = (probs[:, 1] >= threshold).astype(int)\n",
    "\n",
    "# Calculate accuracy and confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_threshold)\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll create another DecisionTreeClassifier model with max_depth=5. \n",
    "Initialize and fit the model below and print the confusion matrix and the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Create and train a new DecisionTreeClassifier model with max_depth=5\n",
    "tree_clf_depth5 = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "tree_clf_depth5.fit(X_train, y_train)\n",
    "\n",
    "# Convert X_test to an array without column names\n",
    "X_test_array = np.array(X_test)\n",
    "\n",
    "# Obtain predicted probabilities\n",
    "probs = tree_clf_depth5.predict_proba(X_test_array)\n",
    "\n",
    "# Apply a threshold of 0.5 for classification\n",
    "threshold = 0.5\n",
    "y_pred_threshold = (probs[:, 1] >= threshold).astype(int)\n",
    "\n",
    "# Calculate accuracy and confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_threshold)\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy with max_depth=5:\", accuracy)\n",
    "print(\"Confusion Matrix with max_depth=5:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see an improvement in the confusion matrix when increasing max_depth to 5? Did you see an improvement in the accuracy score? Write your conclusions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your conclusions here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Add your conclusion here -->\n",
    "++ Improved Accuracy: The accuracy increased from 85.7% to 93.5%.\n",
    "++ Improved Confusion Matrix:\n",
    "    The model now captures positive cases effectively.\n",
    "    False negatives (missed positive cases) dropped from 51 to 17.\n",
    "    There are now 6 false positives (previously zero), indicating a slight trade-off but an overall improvement.\n",
    "++ Increasing max_depth allowed the model to better capture the underlying patterns in the data, improving its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge - Feature Scaling\n",
    "\n",
    "Problem-solving in machine learning is iterative. You can improve your model prediction with various techniques (there is a sweetspot for the time you spend and the improvement you receive though). Now you've completed only one iteration of ML analysis. There are more iterations you can conduct to make improvements. In order to be able to do that, you will need deeper knowledge in statistics and master more data analysis techniques. In this bootcamp, we don't have time to achieve that advanced goal. But you will make constant efforts after the bootcamp to eventually get there.\n",
    "\n",
    "However, now we do want you to learn one of the advanced techniques which is called *feature scaling*. The idea of feature scaling is to standardize/normalize the range of independent variables or features of the data. This can make the outliers more apparent so that you can remove them. This step needs to happen during Challenge 6 after you split the training and test data because you don't want to split the data again which makes it impossible to compare your results with and without feature scaling. For general concepts about feature scaling, click [here](https://en.wikipedia.org/wiki/Feature_scaling). To read deeper, click [here](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e).\n",
    "\n",
    "In the next cell, attempt to improve your model prediction accuracy by means of feature scaling. A library you can utilize is `sklearn.preprocessing.RobustScaler` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)). You'll use the `RobustScaler` to fit and transform your `X_train`, then transform `X_test`. You will use logistic regression to fit and predict your transformed data and obtain the accuracy score in the same way. Compare the accuracy score with your normalized data with the previous accuracy data. Is there an improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Scale the data using RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train a Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy and confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy with feature scaling (RobustScaler):\", accuracy)\n",
    "print(\"Confusion Matrix with feature scaling:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your comments here:\n",
    "\n",
    "Conclusions:\n",
    "\n",
    "++  Accuracy: The accuracy decreased slightly from 93.5% to 91.8% after applying feature scaling and switching to Logistic Regression.\n",
    "++  Confusion Matrix:\n",
    "        False Positives (FP) reduced from 6 to 3.\n",
    "        However, False Negatives (FN) increased from 17 to 26, indicating the model struggles more with identifying positive cases.\n",
    "++  Key Insight: While feature scaling and using Logistic Regression improved precision for the negative class (reducing FP), it introduced more False Negatives, which decreased overall performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
