{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-your-start:\" data-toc-modified-id=\"Before-your-start:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before your start:</a></span></li><li><span><a href=\"#Challenge-1---Explore-The-Dataset\" data-toc-modified-id=\"Challenge-1---Explore-The-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenge 1 - Explore The Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Explore-the-data-from-an-bird's-eye-view.\" data-toc-modified-id=\"Explore-the-data-from-an-bird's-eye-view.-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Explore the data from an bird's-eye view.</a></span></li><li><span><a href=\"#Next,-evaluate-if-the-columns-in-this-dataset-are-strongly-correlated.\" data-toc-modified-id=\"Next,-evaluate-if-the-columns-in-this-dataset-are-strongly-correlated.-2.0.0.2\"><span class=\"toc-item-num\">2.0.0.2&nbsp;&nbsp;</span>Next, evaluate if the columns in this dataset are strongly correlated.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-2---Remove-Column-Collinearity.\" data-toc-modified-id=\"Challenge-2---Remove-Column-Collinearity.-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Challenge 2 - Remove Column Collinearity.</a></span></li><li><span><a href=\"#Challenge-3---Handle-Missing-Values\" data-toc-modified-id=\"Challenge-3---Handle-Missing-Values-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenge 3 - Handle Missing Values</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#In-the-cells-below,-handle-the-missing-values-from-the-dataset.-Remember-to-comment-the-rationale-of-your-decisions.\" data-toc-modified-id=\"In-the-cells-below,-handle-the-missing-values-from-the-dataset.-Remember-to-comment-the-rationale-of-your-decisions.-4.0.0.1\"><span class=\"toc-item-num\">4.0.0.1&nbsp;&nbsp;</span>In the cells below, handle the missing values from the dataset. Remember to comment the rationale of your decisions.</a></span></li><li><span><a href=\"#Again,-examine-the-number-of-missing-values-in-each-column.\" data-toc-modified-id=\"Again,-examine-the-number-of-missing-values-in-each-column.-4.0.0.2\"><span class=\"toc-item-num\">4.0.0.2&nbsp;&nbsp;</span>Again, examine the number of missing values in each column.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-4---Handle-WHOIS_*-Categorical-Data\" data-toc-modified-id=\"Challenge-4---Handle-WHOIS_*-Categorical-Data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Challenge 4 - Handle <code>WHOIS_*</code> Categorical Data</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#In-the-cells-below,-fix-the-country-values-as-intructed-above.\" data-toc-modified-id=\"In-the-cells-below,-fix-the-country-values-as-intructed-above.-5.0.0.1\"><span class=\"toc-item-num\">5.0.0.1&nbsp;&nbsp;</span>In the cells below, fix the country values as intructed above.</a></span></li><li><span><a href=\"#If-a-limited-number-of-values-account-for-the-majority-of-data,-we-can-retain-these-top-values-and-re-label-all-other-rare-values.\" data-toc-modified-id=\"If-a-limited-number-of-values-account-for-the-majority-of-data,-we-can-retain-these-top-values-and-re-label-all-other-rare-values.-5.0.0.2\"><span class=\"toc-item-num\">5.0.0.2&nbsp;&nbsp;</span>If a limited number of values account for the majority of data, we can retain these top values and re-label all other rare values.</a></span></li><li><span><a href=\"#After-verifying,-now-let's-keep-the-top-10-values-of-the-column-and-re-label-other-columns-with-OTHER.\" data-toc-modified-id=\"After-verifying,-now-let's-keep-the-top-10-values-of-the-column-and-re-label-other-columns-with-OTHER.-5.0.0.3\"><span class=\"toc-item-num\">5.0.0.3&nbsp;&nbsp;</span>After verifying, now let's keep the top 10 values of the column and re-label other columns with <code>OTHER</code>.</a></span></li><li><span><a href=\"#In-the-next-cell,-drop-['WHOIS_STATEPRO',-'WHOIS_REGDATE',-'WHOIS_UPDATED_DATE'].\" data-toc-modified-id=\"In-the-next-cell,-drop-['WHOIS_STATEPRO',-'WHOIS_REGDATE',-'WHOIS_UPDATED_DATE'].-5.0.0.4\"><span class=\"toc-item-num\">5.0.0.4&nbsp;&nbsp;</span>In the next cell, drop <code>['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']</code>.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-5---Handle-Remaining-Categorical-Data-&amp;-Convert-to-Ordinal\" data-toc-modified-id=\"Challenge-5---Handle-Remaining-Categorical-Data-&amp;-Convert-to-Ordinal-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge 5 - Handle Remaining Categorical Data &amp; Convert to Ordinal</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#URL-is-easy.-We'll-simply-drop-it-because-it-has-too-many-unique-values-that-there's-no-way-for-us-to-consolidate.\" data-toc-modified-id=\"URL-is-easy.-We'll-simply-drop-it-because-it-has-too-many-unique-values-that-there's-no-way-for-us-to-consolidate.-6.0.0.1\"><span class=\"toc-item-num\">6.0.0.1&nbsp;&nbsp;</span><code>URL</code> is easy. We'll simply drop it because it has too many unique values that there's no way for us to consolidate.</a></span></li><li><span><a href=\"#Print-the-unique-value-counts-of-CHARSET.-You-see-there-are-only-a-few-unique-values.-So-we-can-keep-it-as-it-is.\" data-toc-modified-id=\"Print-the-unique-value-counts-of-CHARSET.-You-see-there-are-only-a-few-unique-values.-So-we-can-keep-it-as-it-is.-6.0.0.2\"><span class=\"toc-item-num\">6.0.0.2&nbsp;&nbsp;</span>Print the unique value counts of <code>CHARSET</code>. You see there are only a few unique values. So we can keep it as it is.</a></span></li><li><span><a href=\"#Before-you-think-of-your-own-solution,-don't-read-the-instructions-that-come-next.\" data-toc-modified-id=\"Before-you-think-of-your-own-solution,-don't-read-the-instructions-that-come-next.-6.0.0.3\"><span class=\"toc-item-num\">6.0.0.3&nbsp;&nbsp;</span>Before you think of your own solution, don't read the instructions that come next.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-6---Modeling,-Prediction,-and-Evaluation\" data-toc-modified-id=\"Challenge-6---Modeling,-Prediction,-and-Evaluation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Challenge 6 - Modeling, Prediction, and Evaluation</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#In-this-lab,-we-will-try-two-different-models-and-compare-our-results.\" data-toc-modified-id=\"In-this-lab,-we-will-try-two-different-models-and-compare-our-results.-7.0.0.1\"><span class=\"toc-item-num\">7.0.0.1&nbsp;&nbsp;</span>In this lab, we will try two different models and compare our results.</a></span></li><li><span><a href=\"#Our-second-algorithm-is-is-DecisionTreeClassifier\" data-toc-modified-id=\"Our-second-algorithm-is-is-DecisionTreeClassifier-7.0.0.2\"><span class=\"toc-item-num\">7.0.0.2&nbsp;&nbsp;</span>Our second algorithm is is DecisionTreeClassifier</a></span></li><li><span><a href=\"#We'll-create-another-DecisionTreeClassifier-model-with-max_depth=5.\" data-toc-modified-id=\"We'll-create-another-DecisionTreeClassifier-model-with-max_depth=5.-7.0.0.3\"><span class=\"toc-item-num\">7.0.0.3&nbsp;&nbsp;</span>We'll create another DecisionTreeClassifier model with max_depth=5.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Bonus-Challenge---Feature-Scaling\" data-toc-modified-id=\"Bonus-Challenge---Feature-Scaling-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Bonus Challenge - Feature Scaling</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will explore a dataset that describes websites with different features and labels them either benign or malicious . We will use supervised learning algorithms to figure out what feature patterns malicious websites are likely to have and use our model to predict malicious websites.\n",
    "\n",
    "Your features will be:\n",
    "\n",
    "+ URL: it is the anonymous identification of the URL analyzed in the study\n",
    "+ URL_LENGTH: it is the number of characters in the URL\n",
    "+ NUMBER_SPECIAL_CHARACTERS: it is number of special characters identified in the URL, such as, “/”, “%”, “#”, “&”, “. “, “=”\n",
    "+ CHARSET: it is a categorical value and its meaning is the character encoding standard (also called character set).\n",
    "+ SERVER: it is a categorical value and its meaning is the operative system of the server got from the packet response.\n",
    "+ CONTENT_LENGTH: it represents the content size of the HTTP header.\n",
    "+ WHOIS_COUNTRY: it is a categorical variable, its values are the countries we got from the server response (specifically, our script used the API of Whois).\n",
    "+ WHOIS_STATEPRO: it is a categorical variable, its values are the states we got from the server response (specifically, our script used the API of Whois).\n",
    "+ WHOIS_REGDATE: Whois provides the server registration date, so, this variable has date values with format DD/MM/YYY HH:MM\n",
    "+ WHOIS_UPDATED_DATE: Through the Whois we got the last update date from the server analyzed\n",
    "+ TCP_CONVERSATION_EXCHANGE: This variable is the number of TCP packets exchanged between the server and our honeypot client\n",
    "+ DIST_REMOTE_TCP_PORT: it is the number of the ports detected and different to TCP\n",
    "+ REMOTE_IPS: this variable has the total number of IPs connected to the honeypot\n",
    "+ APP_BYTES: this is the number of bytes transfered\n",
    "+ SOURCE_APP_PACKETS: packets sent from the honeypot to the server\n",
    "+ REMOTE_APP_PACKETS: packets received from the server\n",
    "+ APP_PACKETS: this is the total number of IP packets generated during the communication between the honeypot and the server\n",
    "+ DNS_QUERY_TIMES: this is the number of DNS packets generated during the communication between the honeypot and the server\n",
    "+ TYPE: this is a categorical variable, its values represent the type of web page analyzed, specifically, 1 is for malicious websites and 0 is for benign websites\n",
    "\n",
    "# Challenge 1 - Explore The Dataset\n",
    "\n",
    "Let's start by exploring the dataset. First load the data file:\n",
    "\n",
    "\n",
    "*Source: [kaggle](https://www.kaggle.com/viratkothari/malicious-and-benign-websites-classification)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = pd.read_csv('../website.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the data from an bird's-eye view.\n",
    "\n",
    "You should already been very familiar with the procedures now so we won't provide the instructions step by step. Reflect on what you did in the previous labs and explore the dataset.\n",
    "\n",
    "Things you'll be looking for:\n",
    "\n",
    "* What the dataset looks like?\n",
    "* What are the data types?\n",
    "* Which columns contain the features of the websites?\n",
    "* Which column contains the feature we will predict? What is the code standing for benign vs malicious websites?\n",
    "* Do we need to transform any of the columns from categorical to ordinal values? If so what are these columns?\n",
    "\n",
    "Feel free to add additional cells for your explorations. Make sure to comment what you find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset to get a glimpse of the data\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(websites.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get a concise summary of the dataset, including the data types and number of non-null entries\n",
    "print(\"Dataset summary (including data types and non-null entries):\")\n",
    "print(websites.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check for any missing values in the dataset\n",
    "print(\"Number of missing values in each column:\")\n",
    "print(websites.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get basic statistics about the numerical columns in the dataset\n",
    "print(\"Basic statistics for numerical columns:\")\n",
    "print(websites.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First 5 Rows of the Dataset:\n",
    "The first 5 rows of the dataset provide a glimpse into the features of each website, including `URL`, `URL_LENGTH`, `NUMBER_SPECIAL_CHARACTERS`, `CHARSET`, `SERVER`, and more. The `Type` column indicates whether the website is benign (0) or malicious (1).\n",
    "\n",
    "### Dataset Summary (Including Data Types and Non-null Entries):\n",
    "- The dataset contains **1781 rows** and **21 columns**.\n",
    "- The columns include:\n",
    "  - Numerical columns: `URL_LENGTH`, `NUMBER_SPECIAL_CHARACTERS`, `CONTENT_LENGTH`, and others.\n",
    "  - Categorical columns: `CHARSET`, `SERVER`, `WHOIS_COUNTRY`, `WHOIS_STATEPRO`, and others.\n",
    "- Some columns have missing values:\n",
    "  - `CHARSET`, `SERVER`, `CONTENT_LENGTH`, `WHOIS_COUNTRY`, `WHOIS_STATEPRO`, `WHOIS_REGDATE`, and `WHOIS_UPDATED_DATE` have missing values.\n",
    "\n",
    "### Number of Missing Values in Each Column:\n",
    "- Columns with missing values include:\n",
    "  - `CHARSET` (7 missing entries)\n",
    "  - `SERVER` (176 missing entries)\n",
    "  - `CONTENT_LENGTH` (812 missing entries)\n",
    "  - `WHOIS_COUNTRY` (306 missing entries)\n",
    "  - `WHOIS_STATEPRO` (362 missing entries)\n",
    "  - `WHOIS_REGDATE` (127 missing entries)\n",
    "  - `WHOIS_UPDATED_DATE` (139 missing entries)\n",
    "  - `DNS_QUERY_TIMES` (1 missing entry)\n",
    "\n",
    "### Basic Statistics for Numerical Columns:\n",
    "- **`URL_LENGTH`**: The length of the URL ranges from 16 to 249 characters.\n",
    "- **`NUMBER_SPECIAL_CHARACTERS`**: The number of special characters in the URL ranges from 5 to 43.\n",
    "- **`CONTENT_LENGTH`**: The content size of the HTTP header ranges from 0 to 649,263 bytes.\n",
    "- **`TCP_CONVERSATION_EXCHANGE`**: The number of TCP packets exchanged ranges from 0 to 1,194.\n",
    "- **`APP_BYTES`**: The number of bytes transferred ranges from 0 to over 2 million.\n",
    "- **`REMOTE_IPS`**: The total number of IPs connected to the honeypot ranges from 0 to 17.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the data types?\n",
    "\n",
    "# Check the data types of each column\n",
    "websites.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types:\n",
    "\n",
    "The data types of each column in the dataset are as follows:\n",
    "\n",
    "- **`object`** (categorical or string values):\n",
    "  - `URL`\n",
    "  - `CHARSET`\n",
    "  - `SERVER`\n",
    "  - `WHOIS_COUNTRY`\n",
    "  - `WHOIS_STATEPRO`\n",
    "  - `WHOIS_REGDATE`\n",
    "  - `WHOIS_UPDATED_DATE`\n",
    "\n",
    "- **`int64`** (integer values):\n",
    "  - `URL_LENGTH`\n",
    "  - `NUMBER_SPECIAL_CHARACTERS`\n",
    "  - `TCP_CONVERSATION_EXCHANGE`\n",
    "  - `DIST_REMOTE_TCP_PORT`\n",
    "  - `REMOTE_IPS`\n",
    "  - `APP_BYTES`\n",
    "  - `SOURCE_APP_PACKETS`\n",
    "  - `REMOTE_APP_PACKETS`\n",
    "  - `SOURCE_APP_BYTES`\n",
    "  - `REMOTE_APP_BYTES`\n",
    "  - `APP_PACKETS`\n",
    "  - `Type`\n",
    "\n",
    "- **`float64`** (floating point values):\n",
    "  - `CONTENT_LENGTH`\n",
    "  - `DNS_QUERY_TIMES`\n",
    "\n",
    "This summary helps to identify which columns are numerical, categorical, or have floating-point data, providing insight into potential preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which columns contain the features of the websites?\n",
    "\n",
    "# List the columns in the dataset to check the exact column names\n",
    "print(\"Column names in the dataset:\")\n",
    "print(websites.columns.tolist())\n",
    "print(\"\\n\")\n",
    "\n",
    "# List the columns that are features (excluding the target column 'Type')\n",
    "features = websites.columns.tolist()\n",
    "\n",
    "# Remove 'Type' column from the list of features\n",
    "features = [col for col in features if col != 'Type']\n",
    "\n",
    "# Show the features\n",
    "print(\"Features (excluding the target column 'Type'):\")\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Names in the Dataset:\n",
    "The dataset contains the following columns:\n",
    "- `URL`\n",
    "- `URL_LENGTH`\n",
    "- `NUMBER_SPECIAL_CHARACTERS`\n",
    "- `CHARSET`\n",
    "- `SERVER`\n",
    "- `CONTENT_LENGTH`\n",
    "- `WHOIS_COUNTRY`\n",
    "- `WHOIS_STATEPRO`\n",
    "- `WHOIS_REGDATE`\n",
    "- `WHOIS_UPDATED_DATE`\n",
    "- `TCP_CONVERSATION_EXCHANGE`\n",
    "- `DIST_REMOTE_TCP_PORT`\n",
    "- `REMOTE_IPS`\n",
    "- `APP_BYTES`\n",
    "- `SOURCE_APP_PACKETS`\n",
    "- `REMOTE_APP_PACKETS`\n",
    "- `SOURCE_APP_BYTES`\n",
    "- `REMOTE_APP_BYTES`\n",
    "- `APP_PACKETS`\n",
    "- `DNS_QUERY_TIMES`\n",
    "- `Type` (target variable)\n",
    "\n",
    "### Features (Excluding the Target Column `'Type'`):\n",
    "The following columns are the features, excluding the target column `'Type'`:\n",
    "- `URL`\n",
    "- `URL_LENGTH`\n",
    "- `NUMBER_SPECIAL_CHARACTERS`\n",
    "- `CHARSET`\n",
    "- `SERVER`\n",
    "- `CONTENT_LENGTH`\n",
    "- `WHOIS_COUNTRY`\n",
    "- `WHOIS_STATEPRO`\n",
    "- `WHOIS_REGDATE`\n",
    "- `WHOIS_UPDATED_DATE`\n",
    "- `TCP_CONVERSATION_EXCHANGE`\n",
    "- `DIST_REMOTE_TCP_PORT`\n",
    "- `REMOTE_IPS`\n",
    "- `APP_BYTES`\n",
    "- `SOURCE_APP_PACKETS`\n",
    "- `REMOTE_APP_PACKETS`\n",
    "- `SOURCE_APP_BYTES`\n",
    "- `REMOTE_APP_BYTES`\n",
    "- `APP_PACKETS`\n",
    "- `DNS_QUERY_TIMES`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which column contains the feature we will predict? What is the code standing for benign vs malicious websites?\n",
    "\n",
    "# The target variable is 'Type'. Let's check the unique values in the 'Type' column\n",
    "print(\"Unique values in the 'Type' column:\")\n",
    "print(websites['Type'].unique())\n",
    "\n",
    "# The 'Type' column contains:\n",
    "# 0 for benign websites\n",
    "# 1 for malicious websites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Column Contains the Feature We Will Predict?\n",
    "The feature we will predict is contained in the **`Type`** column. This column indicates whether a website is benign or malicious.\n",
    "\n",
    "### What is the Code Standing for Benign vs Malicious Websites?\n",
    "- **0** stands for benign websites.\n",
    "- **1** stands for malicious websites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 for bening 1 for maliciuos websites\n",
    "\n",
    "# 0 stands for benign websites\n",
    "# 1 stands for malicious websites\n",
    "print(\"0 stands for benign websites and 1 stands for malicious websites.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we need to transform any of the columns from categorical to ordinal values? If so what are these columns?\n",
    "\n",
    "# List of categorical columns\n",
    "categorical_columns = ['URL', 'CHARSET', 'SERVER', 'WHOIS_COUNTRY', 'WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']\n",
    "\n",
    "# Print unique values of each categorical column to evaluate if they need transformation\n",
    "for col in categorical_columns:\n",
    "    print(f\"Unique values in the '{col}' column:\")\n",
    "    print(websites[col].unique())\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Values in Categorical Columns\n",
    "\n",
    "#### 'URL' column:\n",
    "The 'URL' column contains unique values such as:\n",
    "- ['M0_109', 'B0_2314', 'B0_911', ... 'B0_162', 'B0_1152', 'B0_676']\n",
    "These appear to be unique identifiers and do not require transformation to ordinal values.\n",
    "\n",
    "#### 'CHARSET' column:\n",
    "The 'CHARSET' column contains the following unique values:\n",
    "- ['iso-8859-1', 'UTF-8', 'us-ascii', 'ISO-8859-1', 'utf-8', nan, 'windows-1251', 'ISO-8859', 'windows-1252']\n",
    "This column contains multiple character encodings, but transforming it to ordinal values does not seem appropriate.\n",
    "\n",
    "#### 'SERVER' column:\n",
    "The 'SERVER' column includes a variety of web server identifiers, such as:\n",
    "- ['nginx', 'Apache/2.4.10', 'Microsoft-HTTPAPI/2.0', nan, 'Apache/2', ...]\n",
    "This column contains a diverse set of web server types and should not be transformed to ordinal values.\n",
    "\n",
    "#### 'WHOIS_COUNTRY' column:\n",
    "The 'WHOIS_COUNTRY' column includes country codes and names:\n",
    "- [nan, 'US', 'SC', 'GB', 'UK', 'RU', 'AU', 'CA', 'PA', 'se', ...]\n",
    "There are various country codes and names, including some inconsistencies like 'United Kingdom' and 'GB'. This column may require some cleaning but should not be transformed to ordinal values.\n",
    "\n",
    "#### 'WHOIS_STATEPRO' column:\n",
    "The 'WHOIS_STATEPRO' column contains state or region names:\n",
    "- [nan, 'AK', 'TX', 'Mahe', 'CO', 'FL', 'Kansas', 'Novosibirskaya obl.', ...]\n",
    "This column contains a wide range of state and region names and should remain as a categorical variable.\n",
    "\n",
    "#### 'WHOIS_REGDATE' column:\n",
    "The 'WHOIS_REGDATE' column contains registration dates in the format 'DD/MM/YYYY HH:MM', such as:\n",
    "- ['10/10/2015 18:21', nan, '7/10/1997 4:00', '12/05/1996 0:00', ...]\n",
    "This column contains dates and should be converted to a datetime format.\n",
    "\n",
    "#### 'WHOIS_UPDATED_DATE' column:\n",
    "The 'WHOIS_UPDATED_DATE' column contains update dates in a similar format as 'WHOIS_REGDATE':\n",
    "- [nan, '12/09/2013 0:45', '11/04/2017 0:00', '3/10/2016 3:45', ...]\n",
    "This column contains dates and should also be converted to a datetime format.\n",
    "\n",
    "### Data Cleaning and Date Conversion\n",
    "- **Missing Values in Categorical Columns**: Missing values in the categorical columns such as 'CHARSET', 'SERVER', and 'WHOIS_COUNTRY' should be replaced with a placeholder value (e.g., 'Unknown') to ensure the integrity of the dataset.\n",
    "- **Date Conversion**: The 'WHOIS_REGDATE' and 'WHOIS_UPDATED_DATE' columns contain date information in the format 'DD/MM/YYYY HH:MM'. These should be converted to the datetime data type for proper handling and analysis. This allows for operations like filtering or time-based calculations.\n",
    "\n",
    "### Conclusion:\n",
    "None of the categorical columns need to be transformed into ordinal values. However, the 'WHOIS_REGDATE' and 'WHOIS_UPDATED_DATE' columns should be converted into datetime format for further analysis, and missing values in categorical columns should be handled appropriately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, evaluate if the columns in this dataset are strongly correlated.\n",
    "\n",
    "In the Mushroom supervised learning lab we did recently, we mentioned we are concerned if our dataset has strongly correlated columns because if it is the case we need to choose certain ML algorithms instead of others. We need to evaluate this for our dataset now.\n",
    "\n",
    "Luckily, most of the columns in this dataset are ordinal which makes things a lot easier for us. In the next cells below, evaluate the level of collinearity of the data.\n",
    "\n",
    "We provide some general directions for you to consult in order to complete this step:\n",
    "\n",
    "1. You will create a correlation matrix using the numeric columns in the dataset.\n",
    "\n",
    "1. Create a heatmap using `seaborn` to visualize which columns have high collinearity.\n",
    "\n",
    "1. Comment on which columns you might need to remove due to high collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select only the numeric columns for correlation analysis\n",
    "numeric_columns = websites.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Create a correlation matrix\n",
    "correlation_matrix = websites[numeric_columns].corr()\n",
    "\n",
    "# Create a heatmap to visualize the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Create an empty list to store high correlation pairs\n",
    "high_correlation_pairs = []\n",
    "\n",
    "# Loop through the correlation matrix to find pairs with correlation above 0.9 or below -0.9\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:  # Only consider correlations > 0.9 or < -0.9\n",
    "            high_correlation_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))\n",
    "\n",
    "# Print the high correlation pairs\n",
    "for pair in high_correlation_pairs:\n",
    "    print(f\"{pair[0]} and {pair[1]} have a correlation of {pair[2]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix Analysis\n",
    "\n",
    "The correlation matrix reveals the relationships between the numeric columns in the dataset. The values range from -1 to 1, where:\n",
    "\n",
    "- **1** indicates a perfect positive correlation.\n",
    "- **-1** indicates a perfect negative correlation.\n",
    "- **0** indicates no correlation.\n",
    "\n",
    "#### Key Observations:\n",
    "\n",
    "- **NUMBER_SPECIAL_CHARACTERS** and **URL_LENGTH** have a very high positive correlation of **0.92**. This suggests that these two variables are strongly related, and one of them should be removed to reduce redundancy.\n",
    "- **SOURCE_APP_PACKETS** and **TCP_CONVERSATION_EXCHANGE** have a perfect correlation of **1.00**, indicating that they convey identical information. Removing one would be appropriate to avoid multicollinearity.\n",
    "- **REMOTE_APP_PACKETS** and **TCP_CONVERSATION_EXCHANGE** have a high correlation of **0.99**, suggesting that one of these columns could be dropped to eliminate redundancy.\n",
    "- **REMOTE_APP_PACKETS** and **SOURCE_APP_PACKETS** also share a high correlation of **0.99**, reinforcing the need to drop one of these columns.\n",
    "- **REMOTE_APP_BYTES** and **APP_BYTES** have a perfect correlation of **1.00**, meaning they are redundant and one should be excluded.\n",
    "- **APP_PACKETS** and **TCP_CONVERSATION_EXCHANGE** have a perfect correlation of **1.00**, suggesting one should be removed.\n",
    "- **APP_PACKETS** and **SOURCE_APP_PACKETS** show a perfect correlation of **1.00**, indicating redundancy.\n",
    "- **APP_PACKETS** and **REMOTE_APP_PACKETS** also have a very high correlation of **0.99**, making one of them redundant.\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "- **Highly correlated pairs** to consider removing:\n",
    "  - **NUMBER_SPECIAL_CHARACTERS** and **URL_LENGTH**\n",
    "  - **SOURCE_APP_PACKETS** and **TCP_CONVERSATION_EXCHANGE**\n",
    "  - **REMOTE_APP_PACKETS** and **TCP_CONVERSATION_EXCHANGE**\n",
    "  - **REMOTE_APP_PACKETS** and **SOURCE_APP_PACKETS**\n",
    "  - **REMOTE_APP_BYTES** and **APP_BYTES**\n",
    "  - **APP_PACKETS** and **TCP_CONVERSATION_EXCHANGE**\n",
    "  - **APP_PACKETS** and **SOURCE_APP_PACKETS**\n",
    "  - **APP_PACKETS** and **REMOTE_APP_PACKETS**\n",
    "\n",
    "Removing these columns will help reduce multicollinearity and simplify the dataset for future machine learning model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is an example to know the feacture importance using a ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the XGBoost package, which is a popular library for gradient boosting\n",
    "# Uncomment the following line if you haven't installed XGBoost yet\n",
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for training\n",
    "\n",
    "# X is the feature set with all numeric data except the 'Type' column, which is the target variable\n",
    "X = websites._get_numeric_data().drop('Type', axis=1)\n",
    "\n",
    "# y is the target variable 'Type', which we want to predict\n",
    "y = websites.Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the xgboost library as xgb (I added this cell)\n",
    "# This is required to use the XGBoost model for classification or regression tasks\n",
    "import xgboost as xgb\n",
    "\n",
    "# Initialize the XGBoost model for classification task\n",
    "# The XGBClassifier is used for classification tasks where the target variable is categorical (e.g., 'benign' or 'malicious')\n",
    "xgb_model = xgb.XGBClassifier()  # Suitable for classification tasks like predicting 'benign' or 'malicious'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the XGBoost model on the training data\n",
    "# The model will learn the relationship between the features (X) and the target variable (y)\n",
    "xgb_model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the feature importances in ascending order\n",
    "sort_idx = xgb_model.feature_importances_.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the feature importances using a horizontal bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(X.columns[sort_idx], xgb_model.feature_importances_[sort_idx])\n",
    "plt.title('Feature Importances (XGBoost)', fontsize=16)\n",
    "plt.xlabel('Importance', fontsize=14)\n",
    "plt.ylabel('Features', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Print the features sorted by their importance (from highest to lowest)\n",
    "print(\"Feature importances (from highest to lowest):\")\n",
    "for feature, importance in zip(X.columns[sort_idx[::-1]], xgb_model.feature_importances_[sort_idx[::-1]]):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Plot\n",
    "\n",
    "The plot above shows the importance of each feature in the model. The features are ranked based on their contribution to the prediction. The longer the bar, the higher the importance of the feature. \n",
    "\n",
    "In this case, `REMOTE_APP_PACKETS` and `DIST_REMOTE_TCP_PORT` are the most influential features, while features like `TCP_CONVERSATION_EXCHANGE` and `APP_PACKETS` have lower importance scores. This visualization helps us identify which features play a significant role in the model's predictions and could be further analyzed or optimized. The features `REMOTE_APP_PACKETS` and `DIST_REMOTE_TCP_PORT` show the highest importance in the model, which may indicate their critical role in predicting the target variable. These should be prioritized in future steps of analysis and model optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Remove Column Collinearity.\n",
    "\n",
    "From the heatmap you created, you should have seen at least 3 columns that can be removed due to high collinearity. Remove these columns from the dataset.\n",
    "\n",
    "Note that you should remove as few columns as you can. You don't have to remove all the columns at once. But instead, try removing one column, then produce the heatmap again to determine if additional columns should be removed. As long as the dataset no longer contains columns that are correlated for over 90%, you can stop. Also, keep in mind when two columns have high collinearity, you only need to remove one of them but not both.\n",
    "\n",
    "In the cells below, remove as few columns as you can to eliminate the high collinearity in the dataset. Make sure to comment on your way so that the instructional team can learn about your thinking process which allows them to give feedback. At the end, print the heatmap again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE THE 4 COLUMNS WITH MORE COLLINEARITY\n",
    "\n",
    "# Drop 'TCP_CONVERSATION_EXCHANGE' as it has perfect correlation with 'SOURCE_APP_PACKETS' \n",
    "websites_cleaned = websites.drop(columns=['TCP_CONVERSATION_EXCHANGE'])\n",
    "\n",
    "# Generate and visualize the heatmap again to check if the collinearity is reduced\n",
    "numeric_columns = websites_cleaned.select_dtypes(include=[np.number])  # Ensure we're working with numeric data\n",
    "corr_matrix = numeric_columns.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix After Removing TCP_CONVERSATION_EXCHANGE')\n",
    "plt.show()\n",
    "\n",
    "# Create an empty list to store high correlation pairs\n",
    "high_correlation_pairs = []\n",
    "\n",
    "# Loop through the correlation matrix to find pairs with correlation above 0.9 or below -0.9\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.9:  # Only consider correlations > 0.9 or < -0.9\n",
    "            high_correlation_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "# Print the high correlation pairs\n",
    "for pair in high_correlation_pairs:\n",
    "    print(f\"{pair[0]} and {pair[1]} have a correlation of {pair[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix After Removing TCP_CONVERSATION_EXCHANGE\n",
    "\n",
    "After removing the `TCP_CONVERSATION_EXCHANGE` column, the correlation matrix shows the following updated pairs with high collinearity:\n",
    "\n",
    "- **NUMBER_SPECIAL_CHARACTERS** and **URL_LENGTH** have a correlation of **0.92**. These two variables are highly related, and one should be removed to reduce redundancy.\n",
    "- **REMOTE_APP_PACKETS** and **SOURCE_APP_PACKETS** have a very high correlation of **0.99**, indicating that these columns are almost identical and that one should be removed to avoid multicollinearity.\n",
    "- **REMOTE_APP_BYTES** and **APP_BYTES** have a perfect correlation of **1.00**, meaning they convey identical information and one should be dropped.\n",
    "- **APP_PACKETS** and **SOURCE_APP_PACKETS** also show a perfect correlation of **1.00**, indicating redundancy between these columns.\n",
    "- **APP_PACKETS** and **REMOTE_APP_PACKETS** have a correlation of **0.99**, further reinforcing the need to remove one of them.\n",
    "\n",
    "#### Why We Chose to Remove `TCP_CONVERSATION_EXCHANGE`:\n",
    "\n",
    "- We initially considered removing **SOURCE_APP_PACKETS** as it had a perfect correlation with `TCP_CONVERSATION_EXCHANGE`. However, upon examining the feature importance plot, we observed that `SOURCE_APP_PACKETS` has a higher importance than `TCP_CONVERSATION_EXCHANGE`. Therefore, we opted to remove `TCP_CONVERSATION_EXCHANGE` instead, as it is less impactful to the model based on its feature importance.\n",
    "\n",
    "#### Next Steps:\n",
    "\n",
    "- The next logical step is to remove one of the columns with high collinearity to further reduce redundancy and simplify the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE THE COLUMN WITH THE NEXT HIGHEST CORRELATION (1.00)\n",
    "\n",
    "# Drop 'REMOTE_APP_BYTES' column, as it has perfect correlation with 'APP_BYTES'\n",
    "websites_cleaned = websites_cleaned.drop(columns=['REMOTE_APP_BYTES'])\n",
    "\n",
    "# Generate and visualize the heatmap again to check if the collinearity is reduced\n",
    "numeric_columns = websites_cleaned.select_dtypes(include=[np.number])  # Ensure we're working with numeric data\n",
    "corr_matrix = numeric_columns.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix After Removing REMOTE_APP_BYTES')\n",
    "plt.show()\n",
    "\n",
    "# Create an empty list to store high correlation pairs\n",
    "high_correlation_pairs = []\n",
    "\n",
    "# Loop through the correlation matrix to find pairs with correlation above 0.9 or below -0.9\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.9:  # Only consider correlations > 0.9 or < -0.9\n",
    "            high_correlation_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "# Print the high correlation pairs\n",
    "for pair in high_correlation_pairs:\n",
    "    print(f\"{pair[0]} and {pair[1]} have a correlation of {pair[2]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix After Removing REMOTE_APP_BYTES\n",
    "\n",
    "After removing the `REMOTE_APP_BYTES` column, we can observe the updated correlation matrix where the collinearity has been reduced. The following pairs still show high correlation:\n",
    "\n",
    "- **NUMBER_SPECIAL_CHARACTERS** and **URL_LENGTH** have a correlation of **0.92**, which suggests a high redundancy between these two columns.\n",
    "- **REMOTE_APP_PACKETS** and **SOURCE_APP_PACKETS** have a perfect correlation of **0.99**, indicating they convey almost identical information.\n",
    "- **APP_PACKETS** and **SOURCE_APP_PACKETS** have a perfect correlation of **1.00**, further reinforcing the redundancy between these columns.\n",
    "- **APP_PACKETS** and **REMOTE_APP_PACKETS** also exhibit a very high correlation of **0.99**.\n",
    "\n",
    "#### Why we chose to remove `REMOTE_APP_BYTES`:\n",
    "\n",
    "Among the pair **REMOTE_APP_BYTES** and **APP_BYTES**, we chose to remove **REMOTE_APP_BYTES** because **APP_BYTES** has higher importance based on the feature importance plot. This decision helps us retain the more influential feature while eliminating redundancy.\n",
    "\n",
    "#### Next Steps:\n",
    "\n",
    "- The remaining high correlation pairs should be further analyzed. The next step is to remove another column to reduce multicollinearity and simplify the dataset for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE THE COLUMN WITH THE NEXT HIGHEST CORRELATION (1.00)\n",
    "\n",
    "# Drop 'APP_PACKETS' as it has a perfect correlation with 'SOURCE_APP_PACKETS' \n",
    "# and was removed due to its lower importance in the feature importance plot.\n",
    "websites_cleaned = websites_cleaned.drop(columns=['APP_PACKETS'])\n",
    "\n",
    "# Generate and visualize the heatmap again to check if the collinearity is reduced\n",
    "numeric_columns = websites_cleaned.select_dtypes(include=[np.number])  # Ensure we're working with numeric data\n",
    "corr_matrix = numeric_columns.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix After Removing APP_PACKETS')\n",
    "plt.show()\n",
    "\n",
    "# Create an empty list to store high correlation pairs\n",
    "high_correlation_pairs = []\n",
    "\n",
    "# Loop through the correlation matrix to find pairs with correlation above 0.9 or below -0.9\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.9:  # Only consider correlations > 0.9 or < -0.9\n",
    "            high_correlation_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "# Print the high correlation pairs\n",
    "for pair in high_correlation_pairs:\n",
    "    print(f\"{pair[0]} and {pair[1]} have a correlation of {pair[2]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix After Removing APP_PACKETS\n",
    "\n",
    "After removing the `APP_PACKETS` column, we can see a reduction in collinearity in the dataset. The updated correlation matrix reveals the following:\n",
    "\n",
    "- **NUMBER_SPECIAL_CHARACTERS** and **URL_LENGTH** still have a high correlation of **0.92**. These columns are strongly related, and one of them should be removed to reduce redundancy.\n",
    "- **REMOTE_APP_PACKETS** and **SOURCE_APP_PACKETS** have a very high correlation of **0.99**, indicating significant redundancy. Removing one of them will help to reduce multicollinearity.\n",
    "\n",
    "#### Why we removed `APP_PACKETS` instead of `SOURCE_APP_PACKETS`:\n",
    "In the feature importance plot, `SOURCE_APP_PACKETS` has a higher importance compared to `APP_PACKETS`. Since `APP_PACKETS` and `SOURCE_APP_PACKETS` have a perfect correlation of **1.00**, removing `APP_PACKETS` is a reasonable decision as it has less significance in the prediction model.\n",
    "\n",
    "#### Next Steps:\n",
    "- Consider removing one of the columns with high correlation to further reduce multicollinearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE THE COLUMN WITH THE NEXT HIGHEST CORRELATION (0.99)\n",
    "\n",
    "# Drop 'SOURCE_APP_PACKETS' as it has a high correlation with 'REMOTE_APP_PACKETS' \n",
    "# and was removed due to its lower importance in the feature importance plot.\n",
    "websites_cleaned = websites_cleaned.drop(columns=['SOURCE_APP_PACKETS'])\n",
    "\n",
    "# Generate and visualize the heatmap again to check if the collinearity is reduced\n",
    "numeric_columns = websites_cleaned.select_dtypes(include=[np.number])  # Ensure we're working with numeric data\n",
    "corr_matrix = numeric_columns.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix After Removing SOURCE_APP_PACKETS')\n",
    "plt.show()\n",
    "\n",
    "# Create an empty list to store high correlation pairs\n",
    "high_correlation_pairs = []\n",
    "\n",
    "# Loop through the correlation matrix to find pairs with correlation above 0.9 or below -0.9\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.9:  # Only consider correlations > 0.9 or < -0.9\n",
    "            high_correlation_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "# Print the high correlation pairs\n",
    "for pair in high_correlation_pairs:\n",
    "    print(f\"{pair[0]} and {pair[1]} have a correlation of {pair[2]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix After Removing SOURCE_APP_PACKETS\n",
    "\n",
    "After removing the `SOURCE_APP_PACKETS` column, we can see a reduction in collinearity in the dataset. The updated correlation matrix reveals the following:\n",
    "\n",
    "- **NUMBER_SPECIAL_CHARACTERS** and **URL_LENGTH** still have a high correlation of **0.92**. These columns are strongly related, and one of them should be removed to reduce redundancy.\n",
    "- **REMOTE_APP_PACKETS** and **SOURCE_APP_PACKETS** had a correlation of **0.99**, indicating significant redundancy. Removing one of them will help to reduce multicollinearity.\n",
    "\n",
    "#### Why we removed `SOURCE_APP_PACKETS` instead of `REMOTE_APP_PACKETS`:\n",
    "In the feature importance plot, `REMOTE_APP_PACKETS` has a higher importance compared to `SOURCE_APP_PACKETS`. Since `REMOTE_APP_PACKETS` and `SOURCE_APP_PACKETS` have a perfect correlation of **0.99**, we decided to remove `SOURCE_APP_PACKETS` as it had less significance in the prediction model.\n",
    "\n",
    "#### Next Steps:\n",
    "- Consider removing one of the columns with high correlation to further reduce multicollinearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE THE COLUMN WITH THE NEXT HIGHEST CORRELATION (0.92)\n",
    "\n",
    "# Drop 'URL_LENGTH' as it has a lower importance than 'NUMBER_SPECIAL_CHARACTERS' \n",
    "# and has a high correlation with it.\n",
    "websites_cleaned = websites_cleaned.drop(columns=['URL_LENGTH'])\n",
    "\n",
    "# Generate and visualize the heatmap again to check if the collinearity is reduced\n",
    "numeric_columns = websites_cleaned.select_dtypes(include=[np.number])  # Ensure we're working with numeric data\n",
    "corr_matrix = numeric_columns.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix After Removing URL_LENGTH')\n",
    "plt.show()\n",
    "\n",
    "# Create an empty list to store high correlation pairs\n",
    "high_correlation_pairs = []\n",
    "\n",
    "# Loop through the correlation matrix to find pairs with correlation above 0.9 or below -0.9\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.9:  # Only consider correlations > 0.9 or < -0.9\n",
    "            high_correlation_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "# Print the high correlation pairs\n",
    "for pair in high_correlation_pairs:\n",
    "    print(f\"{pair[0]} and {pair[1]} have a correlation of {pair[2]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix After Removing URL_LENGTH\n",
    "\n",
    "After removing the `URL_LENGTH` column, we can see a reduction in collinearity in the dataset. The updated correlation matrix reveals the following:\n",
    "\n",
    "- **NUMBER_SPECIAL_CHARACTERS** and **URL_LENGTH** had a high correlation of **0.92**, which has now been eliminated by removing the `URL_LENGTH` column.\n",
    "- **REMOTE_APP_PACKETS** and **SOURCE_APP_BYTES** still have a strong correlation, but this is manageable.\n",
    "- The overall collinearity has been reduced, and fewer columns are highly correlated, simplifying the dataset.\n",
    "\n",
    "#### Why we removed `URL_LENGTH` instead of `NUMBER_SPECIAL_CHARACTERS`:\n",
    "`NUMBER_SPECIAL_CHARACTERS` has a higher importance in the feature importance plot than `URL_LENGTH`, making it the more critical variable. Removing `URL_LENGTH` helps reduce redundancy without sacrificing important features for prediction.\n",
    "\n",
    "#### Next Steps:\n",
    "- Consider removing one of the columns with high correlation, such as **REMOTE_APP_PACKETS** and **SOURCE_APP_BYTES**, to further reduce multicollinearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Handle Missing Values\n",
    "\n",
    "The next step would be handling missing values. **We start by examining the number of missing values in each column, which you will do in the next cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Check for missing values in the dataset after dropping columns\n",
    "missing_values = websites_cleaned.isnull().sum()\n",
    "\n",
    "# Display the columns with their missing value count\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Check the percentage of missing values\n",
    "missing_percentage = (missing_values / len(websites_cleaned)) * 100\n",
    "print(\"\\nPercentage of missing values in each column:\")\n",
    "print(missing_percentage[missing_percentage > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will drop the columns with more than 50% of missing data\n",
    "\n",
    "# Drop columns with more than 50% missing data\n",
    "threshold = 0.5  # 50% missing data threshold\n",
    "columns_to_drop = websites_cleaned.columns[websites_cleaned.isnull().mean() > threshold]\n",
    "\n",
    "# Drop those columns\n",
    "websites_cleaned = websites_cleaned.drop(columns=columns_to_drop)\n",
    "\n",
    "# Print the columns that were removed\n",
    "print(f\"Columns removed due to more than 50% missing data: {columns_to_drop.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will drop the rows with missing data\n",
    "\n",
    "# Drop rows with missing data\n",
    "websites_cleaned = websites_cleaned.dropna()\n",
    "\n",
    "# Display the number of rows and columns after dropping the missing data\n",
    "print(\"Shape of dataset after dropping rows with missing data:\", websites_cleaned.shape)\n",
    "\n",
    "# Validate that there are no missing values left\n",
    "assert websites_cleaned.isnull().sum().sum() == 0, \"There are still missing values in the dataset!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of Dataset After Dropping Rows with Missing Data\n",
    "\n",
    "After dropping the rows with missing data, the dataset has been reduced to **636 rows** and **16 columns**. This clean dataset is now ready for further analysis or model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, examine the number of missing values in each column. \n",
    "\n",
    "    If all cleaned, proceed. Otherwise, go back and do more cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine missing values in each column\n",
    "\n",
    "# Examine missing values in each column\n",
    "missing_values_after_cleaning = websites_cleaned.isnull().sum()\n",
    "print(\"Missing values in each column after cleaning:\")\n",
    "print(missing_values_after_cleaning)\n",
    "\n",
    "# Percentage of missing values in each column\n",
    "missing_percentage_after_cleaning = (missing_values_after_cleaning / len(websites_cleaned)) * 100\n",
    "print(\"\\nPercentage of missing values in each column after cleaning:\")\n",
    "print(missing_percentage_after_cleaning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values After Cleaning\n",
    "\n",
    "After removing rows with missing values and columns with more than 50% missing data, we can see that there are no missing values left in the dataset. \n",
    "\n",
    "Here’s the breakdown:\n",
    "\n",
    "- All columns show **0.0%** missing data, which means we have successfully handled the missing values in the dataset.\n",
    "  \n",
    "#### Next Steps:\n",
    "- Proceed with further analysis or model training since the dataset is now clean and free of missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Handle `WHOIS_*` Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several categorical columns we need to handle. These columns are:\n",
    "\n",
    "* `URL`\n",
    "* `CHARSET`\n",
    "* `SERVER`\n",
    "* `WHOIS_COUNTRY`\n",
    "* `WHOIS_STATEPRO`\n",
    "* `WHOIS_REGDATE`\n",
    "* `WHOIS_UPDATED_DATE`\n",
    "\n",
    "How to handle string columns is always case by case. Let's start by working on `WHOIS_COUNTRY`. Your steps are:\n",
    "\n",
    "1. List out the unique values of `WHOIS_COUNTRY`.\n",
    "1. Consolidate the country values with consistent country codes. For example, the following values refer to the same country and should use consistent country code:\n",
    "    * `CY` and `Cyprus`\n",
    "    * `US` and `us`\n",
    "    * `SE` and `se`\n",
    "    * `GB`, `United Kingdom`, and `[u'GB'; u'UK']`\n",
    "\n",
    "#### In the cells below, fix the country values as instructed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "websites_cleaned.WHOIS_COUNTRY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Handle missing values by replacing NaNs with 'None' (or any placeholder value you prefer)\n",
    "websites_cleaned.WHOIS_COUNTRY = websites_cleaned.WHOIS_COUNTRY.fillna('None')\n",
    "\n",
    "good_country = {'None':'None', \n",
    "                'US':'US', \n",
    "                'SC':'SC', \n",
    "                'GB':'UK', \n",
    "                'UK':'UK', \n",
    "                'RU':'RU', \n",
    "                'AU':'AU', \n",
    "                'CA':'CA',\n",
    "                'PA':'PA',\n",
    "                'se':'SE', \n",
    "                'IN':'IN',\n",
    "                'LU':'LU', \n",
    "                'TH':'TH', \n",
    "                \"[u'GB'; u'UK']\":'UK', \n",
    "                'FR':'FR',\n",
    "                'NL':'NL',\n",
    "                'UG':'UG', \n",
    "                'JP':'JP', \n",
    "                'CN':'CN', \n",
    "                'SE':'SE',\n",
    "                'SI':'SI', \n",
    "                'IL':'IL', \n",
    "                'ru':'RU', \n",
    "                'KY':'KY', \n",
    "                'AT':'AT', \n",
    "                'CZ':'CZ', \n",
    "                'PH':'PH', \n",
    "                'BE':'BE', \n",
    "                'NO':'NO', \n",
    "                'TR':'TR', \n",
    "                'LV':'LV',\n",
    "                'DE':'DE', \n",
    "                'ES':'ES', \n",
    "                'BR':'BR', \n",
    "                'us':'US', \n",
    "                'KR':'KR', \n",
    "                'HK':'HK', \n",
    "                'UA':'UA', \n",
    "                'CH':'CH', \n",
    "                'United Kingdom':'UK',\n",
    "                'BS':'BS', \n",
    "                'PK':'PK', \n",
    "                'IT':'IT', \n",
    "                'Cyprus':'CY', \n",
    "                'BY':'BY', \n",
    "                'AE':'AE', \n",
    "                'IE':'IE', \n",
    "                'UY':'UY', \n",
    "                'KG':'KG'}\n",
    "\n",
    "websites_cleaned.WHOIS_COUNTRY = websites_cleaned.WHOIS_COUNTRY.apply(lambda x : good_country[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_cleaned.WHOIS_COUNTRY.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have fixed the country values, can we convert this column to ordinal now?\n",
    "\n",
    "Not yet. If you reflect on the previous labs how we handle categorical columns, you probably remember we ended up dropping a lot of those columns because there are too many unique values. Too many unique values in a column is not desirable in machine learning because it makes prediction inaccurate. But there are workarounds under certain conditions. One of the fixable conditions is:\n",
    "\n",
    "#### If a limited number of values account for the majority of data, we can retain these top values and re-label all other rare values.\n",
    "\n",
    "The `WHOIS_COUNTRY` column happens to be this case. You can verify it by print a bar chart of the `value_counts` in the next cell to verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bar_plot(x, y):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.bar(x, y)\n",
    "    plt.xticks(rotation=90)  # Rotate the x-axis labels\n",
    "    plt.tight_layout()  # Adjust layout to prevent label clipping\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values and their counts\n",
    "print(\"Unique values and their counts:\")\n",
    "print(websites_cleaned.WHOIS_COUNTRY.value_counts())\n",
    "\n",
    "# Now print the bar plot\n",
    "print_bar_plot(websites_cleaned.WHOIS_COUNTRY.unique(), websites_cleaned.WHOIS_COUNTRY.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After verifying, now let's keep the top 10 values of the column and re-label other columns with `OTHER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Keep the top 10 values and re-label other countries as \"OTHER\"\n",
    "top_countries = websites_cleaned.WHOIS_COUNTRY.value_counts().nlargest(10).index\n",
    "websites_cleaned.WHOIS_COUNTRY = websites_cleaned.WHOIS_COUNTRY.apply(lambda x: x if x in top_countries else 'OTHER')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since `WHOIS_COUNTRY` has been re-labelled, we don't need `WHOIS_STATEPRO` any more because the values of the states or provinces may not be relevant any more. We'll drop this column.\n",
    "\n",
    "In addition, we will also drop `WHOIS_REGDATE` and `WHOIS_UPDATED_DATE`. These are the registration and update dates of the website domains. Not of our concerns.\n",
    "\n",
    "#### In the next cell, drop `['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Drop the irrelevant columns\n",
    "websites_cleaned = websites_cleaned.drop(columns=['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Handle Remaining Categorical Data & Convert to Ordinal\n",
    "\n",
    "Now print the `dtypes` of the data again. Besides `WHOIS_COUNTRY` which we already fixed, there should be 3 categorical columns left: `URL`, `CHARSET`, and `SERVER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Print the dtypes of the dataset to see which columns are categorical\n",
    "print(websites_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data types of the dataset\n",
    "\n",
    "After checking the data types of the columns, we see that the following columns are categorical:\n",
    "\n",
    "- `CHARSET`\n",
    "- `SERVER`\n",
    "- `WHOIS_COUNTRY`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `URL` is easy. We'll simply drop it because it has too many unique values that there's no way for us to consolidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'URL' column due to too many unique values\n",
    "websites_cleaned = websites_cleaned.drop(columns=['URL'])\n",
    "\n",
    "# Print the shape of the dataset after dropping the column\n",
    "print(\"Shape of dataset after dropping 'URL':\", websites_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Shape After Cleaning\n",
    "\n",
    "After dropping the 'URL' column and performing the previous cleaning steps, the dataset now has **636 rows** and **12 columns**. This means that:\n",
    "\n",
    "- The dataset now contains 636 data points (or records) after removing rows with missing values.\n",
    "- There are 12 features (or columns) remaining in the dataset after removing the following:\n",
    "\n",
    "  - The `URL` column was dropped due to having too many unique values.\n",
    "  - The `WHOIS_STATEPRO`, `WHOIS_REGDATE`, and `WHOIS_UPDATED_DATE` columns were dropped as they were deemed irrelevant for the analysis.\n",
    "\n",
    "The dataset is now ready for further analysis or model training with the remaining 12 columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the unique value counts of `CHARSET`. You see there are only a few unique values. So we can keep it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Print the unique value counts of 'CHARSET'\n",
    "print(websites_cleaned['CHARSET'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SERVER` is a little more complicated. Print its unique values and think about how you can consolidate those values.\n",
    "\n",
    "#### Before you think of your own solution, don't read the instructions that come next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Print the unique values of the 'SERVER' column to review them\n",
    "websites_cleaned['SERVER'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are so many unique values in the `SERVER` column, there are actually only 3 main server types: `Microsoft`, `Apache`, and `nginx`. Just check if each `SERVER` value contains any of those server types and re-label them. For `SERVER` values that don't contain any of those substrings, label with `Other`.\n",
    "\n",
    "At the end, your `SERVER` column should only contain 4 unique values: `Microsoft`, `Apache`, `nginx`, and `Other`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count `SERVER` value counts here\n",
    "\n",
    "# Re-label the 'SERVER' column based on the presence of substrings\n",
    "websites_cleaned['SERVER'] = websites_cleaned['SERVER'].apply(lambda x: 'Microsoft' if 'Microsoft' in str(x) else \n",
    "                                              ('Apache' if 'Apache' in str(x) else \n",
    "                                               ('nginx' if 'nginx' in str(x) else 'Other')))\n",
    "\n",
    "# Count the values in the 'SERVER' column after re-labeling\n",
    "websites_cleaned['SERVER'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, all our categorical data are fixed now. **Let's convert them to ordinal data using Pandas' `get_dummies` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)).** Make sure you drop the categorical columns by passing `drop_first=True` to `get_dummies` as we don't need them any more. **Also, assign the data with dummy values to a new variable `website_dummy`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Convert categorical columns to dummy variables and create the `websites_dummy` DataFrame\n",
    "websites_dummy = pd.get_dummies(websites_cleaned, drop_first=True)\n",
    "\n",
    "# Display the first few rows of the dataset with dummy variables\n",
    "websites_dummy.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, inspect `website_dummy` to make sure the data and types are intended - there shouldn't be any categorical columns at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Inspect the website_dummy dataframe to ensure there are no categorical columns and check the data types\n",
    "websites_dummy.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 6 - Modeling, Prediction, and Evaluation\n",
    "\n",
    "We'll start off this section by splitting the data to train and test. **Name your 4 variables `X_train`, `X_test`, `y_train`, and `y_test`. Select 80% of the data for training and 20% for testing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = websites_dummy.drop(columns=['Type'])  # Drop the target column 'Type' from the feature set\n",
    "y = websites_dummy['Type']  # The target variable is 'Type'\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of the resulting datasets\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data into Train and Test Sets\n",
    "\n",
    "After splitting the dataset, we have:\n",
    "\n",
    "- **Training data**: 508 samples with 26 features.\n",
    "- **Testing data**: 128 samples with 26 features.\n",
    "\n",
    "These splits are based on an 80/20 ratio, with 80% of the data used for training and 20% for testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this lab, we will try two different models and compare our results.\n",
    "\n",
    "The first model we will use in this lab is logistic regression. We have previously learned about logistic regression as a classification algorithm. In the cell below, load `LogisticRegression` from scikit-learn and initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg_model = LogisticRegression(max_iter=1000)  # Increase max_iter to ensure convergence\n",
    "\n",
    "logreg_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, fit the model to our training data. We have already separated our data into 4 parts. Use those in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the features (X_train and X_test)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Now fit the logistic regression model to the scaled data\n",
    "logreg_model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the Data for Logistic Regression\n",
    "\n",
    "In the previous cell, we scaled the features before fitting the logistic regression model. Scaling is a common technique used in machine learning for several reasons:\n",
    "\n",
    "#### Why Scaling is Important:\n",
    "- **Gradient-based optimization:** Logistic regression, like many other machine learning algorithms, uses gradient-based optimization methods (e.g., the **LBFGS** solver) to minimize the loss function. These optimization methods perform better when the features are on a similar scale. If the features have different scales, the algorithm may have trouble converging, which is why scaling is crucial.\n",
    "  \n",
    "- **Improved performance:** Features with large differences in magnitude (e.g., one feature ranging from 0 to 1 and another from 0 to 1,000) could lead the model to give more importance to the larger values. Scaling helps normalize the range, ensuring that no feature disproportionately influences the model.\n",
    "\n",
    "#### What is Scaling?\n",
    "Scaling (also known as normalization) involves transforming the features so that they are on a similar scale. In this case, we used **StandardScaler** from scikit-learn, which standardizes each feature by removing the mean and scaling to unit variance. This means that each feature will have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "#### How We Scaled the Data:\n",
    "1. **StandardScaler**: We used `StandardScaler` to scale the training data (`X_train`) and testing data (`X_test`). \n",
    "2. **fit_transform()**: We applied `fit_transform()` to the training set, which computes the mean and standard deviation for each feature and scales the data.\n",
    "3. **transform()**: We then applied `transform()` to the testing data, which uses the mean and standard deviation derived from the training data to scale the testing data in the same way.\n",
    "\n",
    "By scaling the data, we ensure that all features contribute equally to the model and that optimization is efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, import `confusion_matrix` and `accuracy_score` from `sklearn.metrics` and fit our testing data. Assign the fitted data to `y_pred` and print the confusion matrix as well as the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "# Import the necessary metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Predict the target variable on the testing data\n",
    "y_pred = logreg_model.predict(X_test_scaled)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy Score: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "After evaluating the logistic regression model using the testing data, we obtained the following results:\n",
    "\n",
    "**Confusion Matrix**:\n",
    "\n",
    "The confusion matrix shows that the model made:\n",
    "- 113 correct predictions for class 0 (True Negatives)\n",
    "- 4 incorrect predictions for class 0 (False Positives)\n",
    "- 6 incorrect predictions for class 1 (False Negatives)\n",
    "- 5 correct predictions for class 1 (True Positives)\n",
    "\n",
    "**Accuracy Score**: \n",
    "- The model achieved an accuracy of **92.19%** on the testing data.\n",
    "\n",
    "These results indicate that the logistic regression model performs well in predicting the target variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are your thoughts on the performance of the model? Write your conclusions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your conclusions here:\n",
    "\n",
    "The logistic regression model performs well with an accuracy of 92.19%. However, the model has made several errors:\n",
    "\n",
    "- **Type 1 errors** (False Positives) occur when the model incorrectly labels a non-malicious website as malicious. This happened **4** times.\n",
    "- **Type 2 errors** (False Negatives) occur when the model incorrectly labels a malicious website as non-malicious. This happened **6** times.\n",
    "\n",
    "Although the model performs well overall, it would be beneficial to investigate ways to minimize these errors, especially False Negatives, as they might have a more significant impact in a real-world scenario, where missing a malicious website could be more harmful than wrongly classifying a non-malicious website as malicious.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our second algorithm is is DecisionTreeClassifier\n",
    "\n",
    "Though is it not required, we will fit a model using the training data and then test the performance of the model using the testing data. Start by loading `DecisionTreeClassifier` from scikit-learn and then initializing and fitting the model. We'll start off with a model where max_depth=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "# Import the DecisionTreeClassifier from scikit-learn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the DecisionTreeClassifier with max_depth=3\n",
    "dt_model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "dt_model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your model, compute the predicted probabilities, decide 0 or 1 using a threshold of 0.5 and print the confusion matrix as well as the accuracy score (on the test set!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "# Compute the predicted probabilities\n",
    "y_pred_prob = dt_model.predict_proba(X_test_scaled)[:, 1]  # Get probabilities for class 1\n",
    "\n",
    "# Decide 0 or 1 using a threshold of 0.5\n",
    "y_pred_dt = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "# Import the necessary metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_dt))\n",
    "\n",
    "# Print the accuracy score\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(f\"\\nAccuracy Score: {accuracy_dt:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model Evaluation\n",
    "\n",
    "After evaluating the Decision Tree model using the testing data, we obtained the following results:\n",
    "\n",
    "**Confusion Matrix**:\n",
    "\n",
    "The confusion matrix shows that the model made:\n",
    "- 117 correct predictions for class 0 (True Negatives)\n",
    "- 0 incorrect predictions for class 0 (False Positives)\n",
    "- 11 incorrect predictions for class 1 (False Negatives)\n",
    "- 0 correct predictions for class 1 (True Positives)\n",
    "\n",
    "**Accuracy Score**: \n",
    "- The model achieved an accuracy of **91.41%** on the testing data.\n",
    "\n",
    "While the accuracy score is high, the model failed to predict any instances of class 1 (malicious websites), as indicated by the 0 True Positives. This suggests that the model might be biased towards class 0 and may require further tuning or a more complex decision tree structure to better handle class 1 predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll create another DecisionTreeClassifier model with max_depth=5. \n",
    "Initialize and fit the model below and print the confusion matrix and the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "# Initialize and fit the DecisionTreeClassifier model with max_depth=5\n",
    "dt_model_5 = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "dt_model_5.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the target variable using the testing data\n",
    "y_pred_5 = dt_model_5.predict(X_test_scaled)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_5))\n",
    "\n",
    "# Print the accuracy score\n",
    "accuracy_5 = accuracy_score(y_test, y_pred_5)\n",
    "print(f\"Accuracy Score: {accuracy_5:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see an improvement in the confusion matrix when increasing max_depth to 5? Did you see an improvement in the accuracy score? Write your conclusions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your conclusions here:\n",
    "\n",
    "### Model Comparison - DecisionTreeClassifier with Different max_depth Values\n",
    "\n",
    "We tested two DecisionTreeClassifier models with different `max_depth` values: one with `max_depth=3` and another with `max_depth=5`. Here are the results:\n",
    "\n",
    "- **Confusion Matrix (max_depth=3)**:\n",
    "    - True Negatives: 117\n",
    "    - False Positives: 0\n",
    "    - False Negatives: 11\n",
    "    - True Positives: 0\n",
    "    - Accuracy: **91.41%**\n",
    "\n",
    "- **Confusion Matrix (max_depth=5)**:\n",
    "    - True Negatives: 115\n",
    "    - False Positives: 2\n",
    "    - False Negatives: 9\n",
    "    - True Positives: 2\n",
    "    - Accuracy: **91.41%**\n",
    "\n",
    "#### Observations:\n",
    "- **Confusion Matrix**: When increasing `max_depth` from 3 to 5, the confusion matrix shows a slight increase in false positives and false negatives. Specifically, we have 2 false positives and 2 true positives for the `max_depth=5` model, compared to no false positives and no true positives in the `max_depth=3` model.\n",
    "- **Accuracy**: There was no change in the accuracy score, remaining at **91.41%** for both models.\n",
    "\n",
    "#### Conclusion:\n",
    "Increasing `max_depth` to 5 did not significantly improve the model's performance. The accuracy remained the same, and while the confusion matrix shows a few changes, the model still struggled with misclassifications for class 1. This suggests that simply increasing the depth does not always lead to better performance. Further tuning or other techniques might be required to improve the model's ability to classify class 1 correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Add your conclusion here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge - Feature Scaling\n",
    "\n",
    "Problem-solving in machine learning is iterative. You can improve your model prediction with various techniques (there is a sweetspot for the time you spend and the improvement you receive though). Now you've completed only one iteration of ML analysis. There are more iterations you can conduct to make improvements. In order to be able to do that, you will need deeper knowledge in statistics and master more data analysis techniques. In this bootcamp, we don't have time to achieve that advanced goal. But you will make constant efforts after the bootcamp to eventually get there.\n",
    "\n",
    "However, now we do want you to learn one of the advanced techniques which is called *feature scaling*. The idea of feature scaling is to standardize/normalize the range of independent variables or features of the data. This can make the outliers more apparent so that you can remove them. This step needs to happen during Challenge 6 after you split the training and test data because you don't want to split the data again which makes it impossible to compare your results with and without feature scaling. For general concepts about feature scaling, click [here](https://en.wikipedia.org/wiki/Feature_scaling). To read deeper, click [here](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e).\n",
    "\n",
    "In the next cell, attempt to improve your model prediction accuracy by means of feature scaling. A library you can utilize is `sklearn.preprocessing.RobustScaler` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)). You'll use the `RobustScaler` to fit and transform your `X_train`, then transform `X_test`. You will use logistic regression to fit and predict your transformed data and obtain the accuracy score in the same way. Compare the accuracy score with your normalized data with the previous accuracy data. Is there an improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Import RobustScaler from sklearn.preprocessing\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Apply RobustScaler to scale the features\n",
    "scaler_robust = RobustScaler()\n",
    "\n",
    "# Fit and transform the training data, then transform the testing data\n",
    "X_train_robust_scaled = scaler_robust.fit_transform(X_train)\n",
    "X_test_robust_scaled = scaler_robust.transform(X_test)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg_model_robust = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit the model to the scaled training data\n",
    "logreg_model_robust.fit(X_train_robust_scaled, y_train)\n",
    "\n",
    "# Predict using the scaled testing data\n",
    "y_pred_robust = logreg_model_robust.predict(X_test_robust_scaled)\n",
    "\n",
    "# Print the confusion matrix and accuracy score for the model with RobustScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_robust))\n",
    "\n",
    "accuracy_robust = accuracy_score(y_test, y_pred_robust)\n",
    "print(f\"\\nAccuracy Score: {accuracy_robust:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your comments here:\n",
    "\n",
    "## Comments:\n",
    "\n",
    "After applying feature scaling with **RobustScaler**, the logistic regression model's performance improved compared to the previous model that used **StandardScaler**. Here's a comparison between the results from both scaling techniques:\n",
    "\n",
    "### Results with **StandardScaler**:\n",
    "- **Confusion Matrix**:\n",
    "    - True Negatives: 117\n",
    "    - False Positives: 0\n",
    "    - False Negatives: 11\n",
    "    - True Positives: 0\n",
    "- **Accuracy Score**: 91.41%\n",
    "\n",
    "### Results with **RobustScaler**:\n",
    "- **Confusion Matrix**:\n",
    "    - True Negatives: 115\n",
    "    - False Positives: 2\n",
    "    - False Negatives: 7\n",
    "    - True Positives: 4\n",
    "- **Accuracy Score**: 92.97%\n",
    "\n",
    "#### Observations:\n",
    "- **Confusion Matrix**: \n",
    "    - With **StandardScaler**, the model struggled with class 1 predictions (malicious websites) with 11 false negatives and no true positives.\n",
    "    - With **RobustScaler**, the model showed improvements, correctly predicting 4 true positives for class 1 and reducing the number of false negatives to 7, but there was a slight increase in false positives.\n",
    "- **Accuracy**: \n",
    "    - The accuracy improved by 1.56 percentage points when using **RobustScaler**, compared to the **StandardScaler** model. This suggests that **RobustScaler** performed better in handling the data, especially in dealing with outliers.\n",
    "\n",
    "#### Conclusion:\n",
    "Feature scaling improved the model's performance, and **RobustScaler** yielded better results than **StandardScaler**, likely due to its ability to handle outliers more effectively. This shows the importance of trying different scaling methods, as each may have different effects on model performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
