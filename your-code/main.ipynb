{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-your-start:\" data-toc-modified-id=\"Before-your-start:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before your start:</a></span></li><li><span><a href=\"#Challenge-1---Explore-The-Dataset\" data-toc-modified-id=\"Challenge-1---Explore-The-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenge 1 - Explore The Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Explore-the-data-from-an-bird's-eye-view.\" data-toc-modified-id=\"Explore-the-data-from-an-bird's-eye-view.-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Explore the data from an bird's-eye view.</a></span></li><li><span><a href=\"#Next,-evaluate-if-the-columns-in-this-dataset-are-strongly-correlated.\" data-toc-modified-id=\"Next,-evaluate-if-the-columns-in-this-dataset-are-strongly-correlated.-2.0.0.2\"><span class=\"toc-item-num\">2.0.0.2&nbsp;&nbsp;</span>Next, evaluate if the columns in this dataset are strongly correlated.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-2---Remove-Column-Collinearity.\" data-toc-modified-id=\"Challenge-2---Remove-Column-Collinearity.-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Challenge 2 - Remove Column Collinearity.</a></span></li><li><span><a href=\"#Challenge-3---Handle-Missing-Values\" data-toc-modified-id=\"Challenge-3---Handle-Missing-Values-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenge 3 - Handle Missing Values</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#In-the-cells-below,-handle-the-missing-values-from-the-dataset.-Remember-to-comment-the-rationale-of-your-decisions.\" data-toc-modified-id=\"In-the-cells-below,-handle-the-missing-values-from-the-dataset.-Remember-to-comment-the-rationale-of-your-decisions.-4.0.0.1\"><span class=\"toc-item-num\">4.0.0.1&nbsp;&nbsp;</span>In the cells below, handle the missing values from the dataset. Remember to comment the rationale of your decisions.</a></span></li><li><span><a href=\"#Again,-examine-the-number-of-missing-values-in-each-column.\" data-toc-modified-id=\"Again,-examine-the-number-of-missing-values-in-each-column.-4.0.0.2\"><span class=\"toc-item-num\">4.0.0.2&nbsp;&nbsp;</span>Again, examine the number of missing values in each column.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-4---Handle-WHOIS_*-Categorical-Data\" data-toc-modified-id=\"Challenge-4---Handle-WHOIS_*-Categorical-Data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Challenge 4 - Handle <code>WHOIS_*</code> Categorical Data</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#In-the-cells-below,-fix-the-country-values-as-intructed-above.\" data-toc-modified-id=\"In-the-cells-below,-fix-the-country-values-as-intructed-above.-5.0.0.1\"><span class=\"toc-item-num\">5.0.0.1&nbsp;&nbsp;</span>In the cells below, fix the country values as intructed above.</a></span></li><li><span><a href=\"#If-a-limited-number-of-values-account-for-the-majority-of-data,-we-can-retain-these-top-values-and-re-label-all-other-rare-values.\" data-toc-modified-id=\"If-a-limited-number-of-values-account-for-the-majority-of-data,-we-can-retain-these-top-values-and-re-label-all-other-rare-values.-5.0.0.2\"><span class=\"toc-item-num\">5.0.0.2&nbsp;&nbsp;</span>If a limited number of values account for the majority of data, we can retain these top values and re-label all other rare values.</a></span></li><li><span><a href=\"#After-verifying,-now-let's-keep-the-top-10-values-of-the-column-and-re-label-other-columns-with-OTHER.\" data-toc-modified-id=\"After-verifying,-now-let's-keep-the-top-10-values-of-the-column-and-re-label-other-columns-with-OTHER.-5.0.0.3\"><span class=\"toc-item-num\">5.0.0.3&nbsp;&nbsp;</span>After verifying, now let's keep the top 10 values of the column and re-label other columns with <code>OTHER</code>.</a></span></li><li><span><a href=\"#In-the-next-cell,-drop-['WHOIS_STATEPRO',-'WHOIS_REGDATE',-'WHOIS_UPDATED_DATE'].\" data-toc-modified-id=\"In-the-next-cell,-drop-['WHOIS_STATEPRO',-'WHOIS_REGDATE',-'WHOIS_UPDATED_DATE'].-5.0.0.4\"><span class=\"toc-item-num\">5.0.0.4&nbsp;&nbsp;</span>In the next cell, drop <code>['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']</code>.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-5---Handle-Remaining-Categorical-Data-&amp;-Convert-to-Ordinal\" data-toc-modified-id=\"Challenge-5---Handle-Remaining-Categorical-Data-&amp;-Convert-to-Ordinal-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge 5 - Handle Remaining Categorical Data &amp; Convert to Ordinal</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#URL-is-easy.-We'll-simply-drop-it-because-it-has-too-many-unique-values-that-there's-no-way-for-us-to-consolidate.\" data-toc-modified-id=\"URL-is-easy.-We'll-simply-drop-it-because-it-has-too-many-unique-values-that-there's-no-way-for-us-to-consolidate.-6.0.0.1\"><span class=\"toc-item-num\">6.0.0.1&nbsp;&nbsp;</span><code>URL</code> is easy. We'll simply drop it because it has too many unique values that there's no way for us to consolidate.</a></span></li><li><span><a href=\"#Print-the-unique-value-counts-of-CHARSET.-You-see-there-are-only-a-few-unique-values.-So-we-can-keep-it-as-it-is.\" data-toc-modified-id=\"Print-the-unique-value-counts-of-CHARSET.-You-see-there-are-only-a-few-unique-values.-So-we-can-keep-it-as-it-is.-6.0.0.2\"><span class=\"toc-item-num\">6.0.0.2&nbsp;&nbsp;</span>Print the unique value counts of <code>CHARSET</code>. You see there are only a few unique values. So we can keep it as it is.</a></span></li><li><span><a href=\"#Before-you-think-of-your-own-solution,-don't-read-the-instructions-that-come-next.\" data-toc-modified-id=\"Before-you-think-of-your-own-solution,-don't-read-the-instructions-that-come-next.-6.0.0.3\"><span class=\"toc-item-num\">6.0.0.3&nbsp;&nbsp;</span>Before you think of your own solution, don't read the instructions that come next.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-6---Modeling,-Prediction,-and-Evaluation\" data-toc-modified-id=\"Challenge-6---Modeling,-Prediction,-and-Evaluation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Challenge 6 - Modeling, Prediction, and Evaluation</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#In-this-lab,-we-will-try-two-different-models-and-compare-our-results.\" data-toc-modified-id=\"In-this-lab,-we-will-try-two-different-models-and-compare-our-results.-7.0.0.1\"><span class=\"toc-item-num\">7.0.0.1&nbsp;&nbsp;</span>In this lab, we will try two different models and compare our results.</a></span></li><li><span><a href=\"#Our-second-algorithm-is-is-DecisionTreeClassifier\" data-toc-modified-id=\"Our-second-algorithm-is-is-DecisionTreeClassifier-7.0.0.2\"><span class=\"toc-item-num\">7.0.0.2&nbsp;&nbsp;</span>Our second algorithm is is DecisionTreeClassifier</a></span></li><li><span><a href=\"#We'll-create-another-DecisionTreeClassifier-model-with-max_depth=5.\" data-toc-modified-id=\"We'll-create-another-DecisionTreeClassifier-model-with-max_depth=5.-7.0.0.3\"><span class=\"toc-item-num\">7.0.0.3&nbsp;&nbsp;</span>We'll create another DecisionTreeClassifier model with max_depth=5.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Bonus-Challenge---Feature-Scaling\" data-toc-modified-id=\"Bonus-Challenge---Feature-Scaling-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Bonus Challenge - Feature Scaling</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will explore a dataset that describes websites with different features and labels them either benign or malicious . We will use supervised learning algorithms to figure out what feature patterns malicious websites are likely to have and use our model to predict malicious websites.\n",
    "\n",
    "Your features will be:\n",
    "\n",
    "+ URL: it is the anonymous identification of the URL analyzed in the study\n",
    "+ URL_LENGTH: it is the number of characters in the URL\n",
    "+ NUMBER_SPECIAL_CHARACTERS: it is number of special characters identified in the URL, such as, “/”, “%”, “#”, “&”, “. “, “=”\n",
    "+ CHARSET: it is a categorical value and its meaning is the character encoding standard (also called character set).\n",
    "+ SERVER: it is a categorical value and its meaning is the operative system of the server got from the packet response.\n",
    "+ CONTENT_LENGTH: it represents the content size of the HTTP header.\n",
    "+ WHOIS_COUNTRY: it is a categorical variable, its values are the countries we got from the server response (specifically, our script used the API of Whois).\n",
    "+ WHOIS_STATEPRO: it is a categorical variable, its values are the states we got from the server response (specifically, our script used the API of Whois).\n",
    "+ WHOIS_REGDATE: Whois provides the server registration date, so, this variable has date values with format DD/MM/YYY HH:MM\n",
    "+ WHOIS_UPDATED_DATE: Through the Whois we got the last update date from the server analyzed\n",
    "+ TCP_CONVERSATION_EXCHANGE: This variable is the number of TCP packets exchanged between the server and our honeypot client\n",
    "+ DIST_REMOTE_TCP_PORT: it is the number of the ports detected and different to TCP\n",
    "+ REMOTE_IPS: this variable has the total number of IPs connected to the honeypot\n",
    "+ APP_BYTES: this is the number of bytes transfered\n",
    "+ SOURCE_APP_PACKETS: packets sent from the honeypot to the server\n",
    "+ REMOTE_APP_PACKETS: packets received from the server\n",
    "+ APP_PACKETS: this is the total number of IP packets generated during the communication between the honeypot and the server\n",
    "+ DNS_QUERY_TIMES: this is the number of DNS packets generated during the communication between the honeypot and the server\n",
    "+ TYPE: this is a categorical variable, its values represent the type of web page analyzed, specifically, 1 is for malicious websites and 0 is for benign websites\n",
    "\n",
    "# Challenge 1 - Explore The Dataset\n",
    "\n",
    "Let's start by exploring the dataset. First load the data file:\n",
    "\n",
    "\n",
    "*Source: [kaggle](https://www.kaggle.com/viratkothari/malicious-and-benign-websites-classification)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = pd.read_csv('../website.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the data from an bird's-eye view.\n",
    "\n",
    "You should already been very familiar with the procedures now so we won't provide the instructions step by step. Reflect on what you did in the previous labs and explore the dataset.\n",
    "\n",
    "Things you'll be looking for:\n",
    "\n",
    "* What the dataset looks like?\n",
    "* What are the data types?\n",
    "* Which columns contain the features of the websites?\n",
    "* Which column contains the feature we will predict? What is the code standing for benign vs malicious websites?\n",
    "* Do we need to transform any of the columns from categorical to ordinal values? If so what are these columns?\n",
    "\n",
    "Feel free to add additional cells for your explorations. Make sure to comment what you find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = pd.read_csv('../website.csv')\n",
    "\n",
    "# General Dataset Information\n",
    "print(\"Dataset Info:\")\n",
    "websites.info()\n",
    "\n",
    "# Displays the first few rows\n",
    "print(\"\\nFirst Few Rows:\")\n",
    "print(websites.head())\n",
    "\n",
    "'''\n",
    "Data consists of 1781 entries and 21 variables. \n",
    "The primary key is the URL (index -> 0) variable.\n",
    "There are 4 object variables (URL, CHARSET, SERVER, WHOIS_COUNTRY, WHOIS_STATEPRO, WHOIS_REGDATE_ AND UPDATED_DATE).\n",
    "There are 2 floating variables (CONTENT_LENGTH, DNS_QUERY_TIMES).\n",
    "The rest, 15 variables, are of integer format (URL_LENGTH, NUMBER_SPECIAL_CHARACTERS, TCP_CONVERSATION_EXCHANGE, DIST_REMOTE_TCP_PORT,REMOTE_IPS,\n",
    "APP_BYTES, SOURCE_APP_PACKETS, REMOTE_APP_PACKETS, SOURCE_APP_BYTES, REMOTE_APP_BYTES, APP_PACKETS)\n",
    "Website features appear in the data under indexes 1-19 (URL_LENGTH through DNS_QUERY_TIMES)\n",
    "The variable to be predicted is Type (index 20). A value of 1 means it's a malicious website, 0 that it is benign)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provides summary information for the numerical columns\n",
    "print(\"\\nNumerical Summary:\")\n",
    "print(websites.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the unique values in each of the categorical columns\n",
    "import pandas as pd\n",
    "\n",
    "# Loads the data\n",
    "websites = pd.read_csv('../website.csv')\n",
    "\n",
    "# Generates the lists of unique values per variable.\n",
    "categorical_cols = websites.select_dtypes(include='object').columns\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nUnique Values in {col}:\")\n",
    "    print(websites[col].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upon evaluating the information from the previous step, it becomes apparent that\n",
    "# the following corrections are necessary for the WHOIS_COUNTRY variable:\n",
    "replacements = {\n",
    "    'us': 'US',\n",
    "    'Cyprus': 'CY',\n",
    "    'ru': 'RU',\n",
    "    'United Kingdom': 'UK',\n",
    "    'se' : 'SE',\n",
    "    \"[u'GB'; u'UK']\" : 'UK'\n",
    "}\n",
    "\n",
    "# Applies the replacements to the WHOIS_COUNTRY column\n",
    "websites['WHOIS_COUNTRY'] = websites['WHOIS_COUNTRY'].replace(replacements)\n",
    "\n",
    "# Verifies the changes\n",
    "print(\"Updated Unique Values in WHOIS_COUNTRY:\")\n",
    "print(websites['WHOIS_COUNTRY'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks the target column (Type)\n",
    "print(\"\\nTarget Column Distribution (Type):\")\n",
    "print(websites['Type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks the correlations between features\n",
    "\n",
    "# Identifies non-numeric columns\n",
    "non_numeric_cols = websites.select_dtypes(include=['object', 'category']).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_cols)\n",
    "\n",
    "# Drops non-numeric columns for correlation calculation purposes\n",
    "data_numeric = websites.drop(columns=non_numeric_cols)\n",
    "\n",
    "# Displays the feature correlations of each categorical variable with Type.\n",
    "print(\"\\nFeature Correlations with Target (Type):\")\n",
    "print(data_numeric.corr()['Type'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, evaluate if the columns in this dataset are strongly correlated.\n",
    "\n",
    "In the Mushroom supervised learning lab we did recently, we mentioned we are concerned if our dataset has strongly correlated columns because if it is the case we need to choose certain ML algorithms instead of others. We need to evaluate this for our dataset now.\n",
    "\n",
    "Luckily, most of the columns in this dataset are ordinal which makes things a lot easier for us. In the next cells below, evaluate the level of collinearity of the data.\n",
    "\n",
    "We provide some general directions for you to consult in order to complete this step:\n",
    "\n",
    "1. You will create a correlation matrix using the numeric columns in the dataset.\n",
    "\n",
    "1. Create a heatmap using `seaborn` to visualize which columns have high collinearity.\n",
    "\n",
    "1. Comment on which columns you might need to remove due to high collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a correlation matrix and a heatmap to visually represent the columns\n",
    "# with high collinearity.\n",
    "\n",
    "# Imports the necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Handles the missing values within numeric columns\n",
    "websites_numeric = websites.select_dtypes(include=['number']).fillna(0)  # Only numeric columns\n",
    "\n",
    "# Creates a correlation matrix\n",
    "correlation_matrix = websites_numeric.corr()\n",
    "\n",
    "# Sets up the matplotlib figure\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Creates a heatmap to visualize the correlation matrix\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', cbar=True, square=True, linewidths=0.5)\n",
    "\n",
    "# Adds title and labels\n",
    "plt.title(\"Correlation Matrix Heatmap\", fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Shows the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is an example to know the feacture importance using a ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splits the data into features (X) and target (y)\n",
    "X = websites._get_numeric_data().drop('Type', axis=1)\n",
    "y = websites.Type\n",
    "\n",
    "# Splits the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "#print(\"Training data shape:\", train_data.shape)\n",
    "#print(\"Testing data shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the necessary libraries\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Initializes the model\n",
    "model = XGBRegressor()\n",
    "\n",
    "# Fits the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Makes predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the necessary libraries\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initializes the classifier\n",
    "xgb_ = XGBClassifier()\n",
    "\n",
    "# Fits the model\n",
    "xgb_.fit(X, y)\n",
    "\n",
    "# Accesses feature importances\n",
    "feature_importances = xgb_.feature_importances_\n",
    "\n",
    "# Ensures feature_importances aligns with X.columns\n",
    "assert len(feature_importances) == X.shape[1], \"Mismatch between feature importances and feature columns.\"\n",
    "\n",
    "# Sorts feature importances\n",
    "sort_idx = feature_importances.argsort()\n",
    "sorted_features = X.columns[sort_idx]\n",
    "sorted_importances = feature_importances[sort_idx]\n",
    "\n",
    "# Plots the bar chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(sorted_features, sorted_importances, color=\"skyblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"XGBoost Feature Importances\")\n",
    "\n",
    "# Annotates each bar with its value\n",
    "for i, value in enumerate(sorted_importances):\n",
    "    plt.text(value, i, f\"{value:.2f}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Outputs the sorted feature importances\n",
    "print(\"Sorted Feature Importances:\", sorted_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In the previous plot we can see the feactures with lower weight in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Remove Column Collinearity.\n",
    "\n",
    "From the heatmap you created, you should have seen at least 3 columns that can be removed due to high collinearity. Remove these columns from the dataset.\n",
    "\n",
    "Note that you should remove as few columns as you can. You don't have to remove all the columns at once. But instead, try removing one column, then produce the heatmap again to determine if additional columns should be removed. As long as the dataset no longer contains columns that are correlated for over 90%, you can stop. Also, keep in mind when two columns have high collinearity, you only need to remove one of them but not both.\n",
    "\n",
    "In the cells below, remove as few columns as you can to eliminate the high collinearity in the dataset. Make sure to comment on your way so that the instructional team can learn about your thinking process which allows them to give feedback. At the end, print the heatmap again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports the necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes columns to eliminate the high collinearity in the dataset\n",
    "\n",
    "# Excludes non-numeric columns from the correlation calculation\n",
    "# and displays the list of columns for verification\n",
    "cols_for_correll = websites.select_dtypes (exclude = ['object']).columns\n",
    "print(\"Numeric columns used in the correlation matrix\", cols_for_correll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While a correlation matrix had previously been generated, this code recreates it without non-numeric columns.\n",
    "websites_numeric = websites.drop(columns=['URL'], errors='ignore').select_dtypes(include=['number'])\n",
    "correlation_matrix = websites_numeric.corr()\n",
    "\n",
    "# Identifies the pairs of columns with correlation > 0.9\n",
    "high_corr_pairs = correlation_matrix.where((correlation_matrix > 0.9) & (correlation_matrix < 1.0)).stack()\n",
    "high_corr_pairs.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "# Displays the pairs with correlation greater than 0.9\n",
    "print(\"Highly Correlated Pairs (Correlation > 0.9):\")\n",
    "print(high_corr_pairs)\n",
    "\n",
    "# Iteratively removes the 4 columns with the most collinearity, excluding those that can be kept.\n",
    "columns_to_remove = []\n",
    "for _ in range(4):  \n",
    "    # Finds the column with the most correlations above the threshold\n",
    "    col_to_remove = (\n",
    "        correlation_matrix.abs().sum().sort_values(ascending=False).index[0]\n",
    "    )\n",
    "    columns_to_remove.append(col_to_remove)\n",
    "    \n",
    "    # Drops the selected column from the correlation matrix\n",
    "    correlation_matrix = correlation_matrix.drop(index=col_to_remove, columns=col_to_remove)\n",
    "\n",
    "print(\"Columns Removed Due to High Collinearity:\", columns_to_remove)\n",
    "\n",
    "# Removes the identified columns from the dataset\n",
    "websites_reduced = websites_numeric.drop(columns=columns_to_remove)\n",
    "\n",
    "# Generates the new correlation matrix and heatmap\n",
    "correlation_matrix_reduced = websites_reduced.corr()\n",
    "\n",
    "# Visualizes the updated heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix_reduced, annot=True, fmt='.2f', cmap='coolwarm', cbar=True, square=True, linewidths=0.5)\n",
    "plt.title(\"Updated Correlation Matrix Heatmap (After Removing High Collinearity)\", fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Checks for remaining highly correlated pairs\n",
    "remaining_high_corr_pairs = correlation_matrix_reduced.where(\n",
    "    (correlation_matrix_reduced > 0.9) & (correlation_matrix_reduced < 1.0)\n",
    ").stack()\n",
    "remaining_high_corr_pairs.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "# Displays the remaining highly correlated pairs for verification.\n",
    "print(\"Remaining Highly Correlated Pairs (Correlation > 0.9):\")\n",
    "print(remaining_high_corr_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Handle Missing Values\n",
    "\n",
    "The next step would be handling missing values. **We start by examining the number of missing values in each column, which you will do in the next cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks for missing values in the data\n",
    "print(\"\\nMissing Values Per Column:\")\n",
    "missing_values = websites.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Calculate the threshold for missing values (50% of rows)\n",
    "threshold = len(websites) * 0.5\n",
    "\n",
    "# Identifies columns with more than 50% missing data\n",
    "columns_to_drop = missing_values[missing_values > threshold].index\n",
    "print(\"\\nColumns with More Than 50% Missing Data:\", columns_to_drop.tolist())\n",
    "\n",
    "# Drops columns with more than 50% missing data\n",
    "websites_cleaned = websites.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops the columns with more than 50% of missing data\n",
    "threshold = len(websites) * 0.5\n",
    "\n",
    "# Identifies columns with more than 50% missing data\n",
    "columns_to_drop = missing_values[missing_values > threshold].index\n",
    "print(\"\\nColumns with More Than 50% Missing Data:\", columns_to_drop.tolist())\n",
    "\n",
    "# Drops columns with more than 50% missing data\n",
    "websites_cleaned = websites.drop(columns=columns_to_drop)\n",
    "\n",
    "# Displays the cleaned dataset's columns\n",
    "print(\"\\nRemaining Columns After Dropping:\")\n",
    "print(websites_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops rows with missing data\n",
    "websites_cleaned = websites_cleaned.dropna()\n",
    "\n",
    "# Displays the number of rows after dropping missing rows\n",
    "print(\"\\nRemaining Rows After Dropping Rows with Missing Data:\")\n",
    "print(len(websites_cleaned))\n",
    "\n",
    "# Verifies that there are no remaining missing values\n",
    "print(\"\\nMissing Values After Cleaning:\")\n",
    "print(websites_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, examine the number of missing values in each column. \n",
    "\n",
    "    If all cleaned, proceed. Otherwise, go back and do more cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The table displayed above shows everything has now been cleaned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Handle `WHOIS_*` Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several categorical columns we need to handle. These columns are:\n",
    "\n",
    "* `URL`\n",
    "* `CHARSET`\n",
    "* `SERVER`\n",
    "* `WHOIS_COUNTRY`\n",
    "* `WHOIS_STATEPRO`\n",
    "* `WHOIS_REGDATE`\n",
    "* `WHOIS_UPDATED_DATE`\n",
    "\n",
    "How to handle string columns is always case by case. Let's start by working on `WHOIS_COUNTRY`. Your steps are:\n",
    "\n",
    "1. List out the unique values of `WHOIS_COUNTRY`.\n",
    "1. Consolidate the country values with consistent country codes. For example, the following values refer to the same country and should use consistent country code:\n",
    "    * `CY` and `Cyprus`\n",
    "    * `US` and `us`\n",
    "    * `SE` and `se`\n",
    "    * `GB`, `United Kingdom`, and `[u'GB'; u'UK']`\n",
    "\n",
    "#### In the cells below, fix the country values as intructed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the recoding done on the WHOIS_COUNTRY variable during data exploration above (for referential purposes).\n",
    "'''\n",
    "# The following corrections are necessary for the WHOIS_COUNTRY variable:\n",
    "replacements = {\n",
    "    'us': 'US',\n",
    "    'Cyprus': 'CY',\n",
    "    'ru': 'RU',\n",
    "    'se': 'SE',\n",
    "    'United Kingdom': 'UK',\n",
    "    \"[u'GB'; u'UK']\" : 'UK'\n",
    "}\n",
    "\n",
    "# Apply the replacements to the WHOIS_COUNTRY column\n",
    "websites['WHOIS_COUNTRY'] = websites['WHOIS_COUNTRY'].replace(replacements)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Updated Unique Values in WHOIS_COUNTRY:\")\n",
    "print(websites['WHOIS_COUNTRY'].unique())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lists the unique values in the WHOIS_COUNTRY column\n",
    "unique_values = websites['WHOIS_COUNTRY'].unique()\n",
    "print(\"Unique Values in WHOIS_COUNTRY:\")\n",
    "print(unique_values)\n",
    "\n",
    "websites.WHOIS_COUNTRY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists the unique values in the WHOIS_COUNTRY column, for verification purposes.\n",
    "# The idea is to confirm whether the recoding was done properly.\n",
    "websites.WHOIS_COUNTRY.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have fixed the country values, can we convert this column to ordinal now?\n",
    "\n",
    "Not yet. If you reflect on the previous labs how we handle categorical columns, you probably remember we ended up dropping a lot of those columns because there are too many unique values. Too many unique values in a column is not desirable in machine learning because it makes prediction inaccurate. But there are workarounds under certain conditions. One of the fixable conditions is:\n",
    "\n",
    "#### If a limited number of values account for the majority of data, we can retain these top values and re-label all other rare values.\n",
    "\n",
    "The `WHOIS_COUNTRY` column happens to be this case. You can verify it by print a bar chart of the `value_counts` in the next cell to verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gets the value counts for WHOIS_COUNTRY\n",
    "country_counts = websites['WHOIS_COUNTRY'].value_counts()\n",
    "\n",
    "# Calls the function with aligned data\n",
    "print_bar_plot(country_counts.index, country_counts.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After verifying, now let's keep the top 10 values of the column and re-label other columns with `OTHER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifies the top 10 most frequent values in WHOIS_COUNTRY\n",
    "top_10_countries = websites['WHOIS_COUNTRY'].value_counts().nlargest(10).index\n",
    "\n",
    "# Re-labels all other values as 'OTHER'\n",
    "websites['WHOIS_COUNTRY'] = websites['WHOIS_COUNTRY'].apply(\n",
    "    lambda x: x if x in top_10_countries else 'OTHER'\n",
    ")\n",
    "\n",
    "# Displays the updated column, for verification purposes.\n",
    "print(\"Updated WHOIS_COUNTRY Value Counts:\")\n",
    "print(websites['WHOIS_COUNTRY'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since `WHOIS_COUNTRY` has been re-labelled, we don't need `WHOIS_STATEPRO` any more because the values of the states or provinces may not be relevant any more. We'll drop this column.\n",
    "\n",
    "In addition, we will also drop `WHOIS_REGDATE` and `WHOIS_UPDATED_DATE`. These are the registration and update dates of the website domains. Not of our concerns.\n",
    "\n",
    "#### In the next cell, drop `['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops 3 variables: WHOIS_STATEPRO, WHOIS_REGDATE4 AND WHOIS_UPDATED_DATE\n",
    "# Drops the specified columns\n",
    "columns_to_drop = ['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']\n",
    "websites = websites.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Verifies the updated dataframe\n",
    "print(\"Remaining Columns:\")\n",
    "print(websites.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Handle Remaining Categorical Data & Convert to Ordinal\n",
    "\n",
    "Now print the `dtypes` of the data again. Besides `WHOIS_COUNTRY` which we already fixed, there should be 3 categorical columns left: `URL`, `CHARSET`, and `SERVER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the data types of the remaining variables\n",
    "print(\"Data Types of Remaining Columns\")\n",
    "print(websites.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `URL` is easy. We'll simply drop it because it has too many unique values that there's no way for us to consolidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops the URL column\n",
    "websites = websites.drop(columns=['URL'], errors='ignore')\n",
    "\n",
    "# Verifies the updated dataframe\n",
    "print(\"Columns After Dropping 'URL':\")\n",
    "print(websites.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the unique value counts of `CHARSET`. You see there are only a few unique values. So we can keep it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the unique value counts of CHARSET\n",
    "unique_charset = websites['CHARSET'].unique()\n",
    "print(\"Unique Values in CHARSET:\")\n",
    "print(unique_charset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SERVER` is a little more complicated. Print its unique values and think about how you can consolidate those values.\n",
    "\n",
    "#### Before you think of your own solution, don't read the instructions that come next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the unique value counts of SERVER\n",
    "unique_server = websites['SERVER'].unique()\n",
    "print(\"Unique Values in SERVER:\")\n",
    "print(unique_server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are so many unique values in the `SERVER` column, there are actually only 3 main server types: `Microsoft`, `Apache`, and `nginx`. Just check if each `SERVER` value contains any of those server types and re-label them. For `SERVER` values that don't contain any of those substrings, label with `Other`.\n",
    "\n",
    "At the end, your `SERVER` column should only contain 4 unique values: `Microsoft`, `Apache`, `nginx`, and `Other`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks each entry's SERVER value and recodes as needed to leave only the following categories:\n",
    "# Microsoft, Apache, nginx and Other\n",
    "\n",
    "# Defines a function to re-label the values in the SERVER column\n",
    "def categorize_server(server):\n",
    "    server = str(server).lower()  # Convert to lowercase for consistent comparison\n",
    "    if 'microsoft' in server:\n",
    "        return 'Microsoft'\n",
    "    elif 'apache' in server:\n",
    "        return 'Apache'\n",
    "    elif 'nginx' in server:\n",
    "        return 'nginx'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Applies the function to the SERVER column\n",
    "websites['SERVER'] = websites['SERVER'].apply(categorize_server)\n",
    "\n",
    "# Verifies the re-labeled values\n",
    "print(\"Unique Values in SERVER After Re-labeling:\")\n",
    "print(websites['SERVER'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, all our categorical data are fixed now. **Let's convert them to ordinal data using Pandas' `get_dummies` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)).** Make sure you drop the categorical columns by passing `drop_first=True` to `get_dummies` as we don't need them any more. **Also, assign the data with dummy values to a new variable `website_dummy`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts categorical columns to dummy/ordinal data\n",
    "website_dummy = pd.get_dummies(websites, drop_first=True)\n",
    "\n",
    "# Verifies the new dataframe\n",
    "print(\"Dummy-Encoded DataFrame (website_dummy):\")\n",
    "print(website_dummy.head())\n",
    "\n",
    "# Checks the updated column names\n",
    "print(\"\\nColumns in Dummy-Encoded DataFrame:\")\n",
    "print(website_dummy.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, inspect `website_dummy` to make sure the data and types are intended - there shouldn't be any categorical columns at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspects the data types of website_dummy\n",
    "print(\"Data Types in website_dummy:\")\n",
    "print(website_dummy.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 6 - Modeling, Prediction, and Evaluation\n",
    "\n",
    "We'll start off this section by splitting the data to train and test. **Name your 4 variables `X_train`, `X_test`, `y_train`, and `y_test`. Select 80% of the data for training and 20% for testing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the data to train and test\n",
    "\n",
    "# Imports the necessary library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splits the data into features (X) and target (y)\n",
    "X = websites._get_numeric_data().drop('Type', axis=1)\n",
    "y = websites.Type\n",
    "\n",
    "# Splits the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Drops rows with missing values\n",
    "X_train = X_train.dropna()\n",
    "y_train = y_train[X_train.index]\n",
    "\n",
    "# Checks if there are any missing values\n",
    "print(\"Missing Values in X_train After Dropping Rows:\")\n",
    "print(X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this lab, we will try two different models and compare our results.\n",
    "\n",
    "The first model we will use in this lab is logistic regression. We have previously learned about logistic regression as a classification algorithm. In the cell below, load `LogisticRegression` from scikit-learn and initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads LogisticRegression and initializes the model.\n",
    "\n",
    "# Import the necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Prepares the features (X) and target (y)\n",
    "X = website_dummy.drop(columns=['Type'])  # Replace 'Type' with your actual target column\n",
    "y = website_dummy['Type']\n",
    "\n",
    "# Splits the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handles missing values using imputation (to try it out)\n",
    "imputer = SimpleImputer(strategy='mean')  # You can use 'median' or 'most_frequent' as needed\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# Scales the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initializes the logistic regression model with an increased max_iter\n",
    "logistic_model = LogisticRegression(random_state=42, max_iter=5000)  # Increased max_iter\n",
    "\n",
    "# Fits the model to the scaled training data\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Displays the model's configuration\n",
    "print(logistic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, fit the model to our training data. We have already separated our data into 4 parts. Use those in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the model to the training data\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Displays the model's configuration\n",
    "print(logistic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, import `confusion_matrix` and `accuracy_score` from `sklearn.metrics` and fit our testing data. Assign the fitted data to `y_pred` and print the confusion matrix as well as the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports confusion_matrix and accuracy_score from sklearn.metrics and fits the test data.\n",
    "# Also, the fitted data is assigned to y_pred. Both the matrix and the score are displayed.\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Predicts the test set\n",
    "y_pred = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "# Generates the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculates the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Prints the confusion matrix and accuracy score\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(f\"{accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are your thoughts on the performance of the model? Write your conclusions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your conclusions here:\n",
    "    The model has high overall accuracy, but there are some nuances that may be work looking into.\n",
    "\n",
    "    There are 24 true positives, 305 true negatives, 1 false positive and 27 false negatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our second algorithm is is DecisionTreeClassifier\n",
    "\n",
    "Though is it not required, we will fit a model using the training data and then test the performance of the model using the testing data. Start by loading `DecisionTreeClassifier` from scikit-learn and then initializing and fitting the model. We'll start off with a model where max_depth=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads DecisionTreeClassifier. Initializes and fits the model (with max_depth=3)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Initialize and fit the Decision Tree model with max_depth=3\n",
    "decision_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "decision_tree.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicts on the test set\n",
    "y_pred = decision_tree.predict(X_test_scaled)\n",
    "\n",
    "# Evaluates the model\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Displays the results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(f\"{accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your model, compute the predicted probabilities, decide 0 or 1 using a threshold of 0.5 and print the confusion matrix as well as the accuracy score (on the test set!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests the model by computing the predicted probabilities and printing the confusion matrix and accuracy score.\n",
    "\n",
    "# Import the necessary library\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Computes the predicted probabilities for the positive class (1)\n",
    "y_prob = decision_tree.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Applies the threshold of 0.5 to classify as 0 or 1\n",
    "y_pred_thresholded = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "# Generates the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_thresholded)\n",
    "\n",
    "# Calculates the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred_thresholded)\n",
    "\n",
    "# Prints the results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(f\"{accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll create another DecisionTreeClassifier model with max_depth=5. \n",
    "Initialize and fit the model below and print the confusion matrix and the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads DecisionTreeClassifier. Initializes and fits the model (with max_depth=3)\n",
    "\n",
    "# Loads the necessary library\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initializes and fits the Decision Tree model with max_depth=3\n",
    "decision_tree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "decision_tree.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicts on the test set\n",
    "y_pred = decision_tree.predict(X_test_scaled)\n",
    "\n",
    "# Evaluates the model\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Displays the results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(f\"{accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see an improvement in the confusion matrix when increasing max_depth to 5? Did you see an improvement in the accuracy score? Write your conclusions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your conclusions here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results to the comparison between both models is as follow: \n",
    "\n",
    "-The number of true positives increased from 15 to 25, which is an improvement.\n",
    "\n",
    "-True negatives decreased from 306 to 303 and the number of false positives increased from 0 to 3, but it could be considered a trade off in light of the improvement in true positives.\n",
    "\n",
    "-False negatives decreased from 36 to 26. This means that the model is less likely to miss positive cases.\n",
    "\n",
    "-The accuracy increased slightly from 0.90 to 0.92.\n",
    "\n",
    "Increasing the max_depth from 3 to 5 improved accuracy (slightly) and the model's ability to identify true positives, so the max_depth = 5 seems to be more fitting for this dataset. Cross-validation or metrics such as the F1 score could provide additional insights to support (or reject) this conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge - Feature Scaling\n",
    "\n",
    "Problem-solving in machine learning is iterative. You can improve your model prediction with various techniques (there is a sweetspot for the time you spend and the improvement you receive though). Now you've completed only one iteration of ML analysis. There are more iterations you can conduct to make improvements. In order to be able to do that, you will need deeper knowledge in statistics and master more data analysis techniques. In this bootcamp, we don't have time to achieve that advanced goal. But you will make constant efforts after the bootcamp to eventually get there.\n",
    "\n",
    "However, now we do want you to learn one of the advanced techniques which is called *feature scaling*. The idea of feature scaling is to standardize/normalize the range of independent variables or features of the data. This can make the outliers more apparent so that you can remove them. This step needs to happen during Challenge 6 after you split the training and test data because you don't want to split the data again which makes it impossible to compare your results with and without feature scaling. For general concepts about feature scaling, click [here](https://en.wikipedia.org/wiki/Feature_scaling). To read deeper, click [here](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e).\n",
    "\n",
    "In the next cell, attempt to improve your model prediction accuracy by means of feature scaling. A library you can utilize is `sklearn.preprocessing.RobustScaler` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)). You'll use the `RobustScaler` to fit and transform your `X_train`, then transform `X_test`. You will use logistic regression to fit and predict your transformed data and obtain the accuracy score in the same way. Compare the accuracy score with your normalized data with the previous accuracy data. Is there an improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments with feature scaling\n",
    "\n",
    "# Imports the necessary libraries\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Initializes the RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Scales the data \n",
    "X_train_robust = robust_scaler.fit_transform(X_train)\n",
    "X_test_robust = robust_scaler.transform(X_test)\n",
    "\n",
    "# Initializes and fits the logistic regression model\n",
    "logistic_model_robust = LogisticRegression(random_state=42, max_iter=5000)\n",
    "logistic_model_robust.fit(X_train_robust, y_train)\n",
    "\n",
    "# Predicts on the robustly scaled test set\n",
    "y_pred_robust = logistic_model_robust.predict(X_test_robust)\n",
    "\n",
    "# Evaluates the model\n",
    "conf_matrix_robust = confusion_matrix(y_test, y_pred_robust)\n",
    "accuracy_robust = accuracy_score(y_test, y_pred_robust)\n",
    "\n",
    "# Prints the results\n",
    "print(\"Confusion Matrix with RobustScaler:\")\n",
    "print(conf_matrix_robust)\n",
    "print(\"\\nAccuracy Score with RobustScaler:\")\n",
    "print(f\"{accuracy_robust:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new model had similar recall and accuracy as the model with max_depth=5, and had slightly fewer false positives(1). This indicates better precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
