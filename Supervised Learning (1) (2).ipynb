{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = pd.read_csv(r\"C:\\Users\\sombe\\Downloads\\website.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the dataset looks like?\n",
    "print('This is the shape of the dataset:\\n',websites.shape)\n",
    "print('These are the columns of the websites dataset:\\n',websites.columns)\n",
    "print('The column Type contains the target variable') \n",
    "print('The data types are:\\n', websites.dtypes)\n",
    "unique_types = websites['Type'].unique()\n",
    "print('The code standing for bening and malicious is: ', unique_types)\n",
    "# 0 for bening 1 for maliciuos websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we need to transform any of the columns from categorical to ordinal values? If so what are these columns?\n",
    "\n",
    "#All Columns with dtype: object Need Transformation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_websites = websites.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = numeric_websites.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', linewidths=0.5, vmin=-1, vmax=1)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = websites._get_numeric_data().drop('Type', axis=1)\n",
    "y = websites.Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Assuming you have already trained a model\n",
    "# xgb_model = xgb.train(...)\n",
    "\n",
    "# Get feature importance\n",
    "importance = xgb_model.feature_importances_\n",
    "\n",
    "# Print feature importance\n",
    "print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'max_depth': 4,\n",
    "    'eta': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=10)\n",
    "\n",
    "# Plot feature importance\n",
    "xgb.plot_importance(xgb_model)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In the previous plot we can see the feactures with lower weight in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to remove highly correlated columns\n",
    "def remove_highly_correlated_columns(data, threshold=0.9):\n",
    "    correlation_matrix = data.corr()\n",
    "    high_corr_pairs = [(i, j) for i in correlation_matrix.columns for j in correlation_matrix.columns \n",
    "                       if (i != j) and abs(correlation_matrix.loc[i, j]) > threshold]\n",
    "\n",
    "    if high_corr_pairs:\n",
    "        print(\"Highly correlated pairs detected:\")\n",
    "        for pair in high_corr_pairs:\n",
    "            print(f\"{pair[0]} and {pair[1]} have a correlation of {correlation_matrix.loc[pair[0], pair[1]]:.2f}\")\n",
    "        \n",
    "        # Remove the first column of the first detected pair\n",
    "        column_to_remove = high_corr_pairs[0][0]\n",
    "        print(f\"\\nRemoving column: {column_to_remove}\")\n",
    "        data = data.drop(columns=[column_to_remove])\n",
    "        \n",
    "        # Recompute the heatmap\n",
    "        correlation_matrix = data.corr()\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', linewidths=0.5, vmin=-1, vmax=1)\n",
    "        plt.title(\"Updated Feature Correlation Heatmap\")\n",
    "        plt.show()\n",
    "        \n",
    "        return data\n",
    "    else:\n",
    "        print(\"No highly correlated columns detected.\")\n",
    "        return data\n",
    "\n",
    "# Your dataset: Assuming `websites` is your DataFrame\n",
    "websites_cleaned = websites.select_dtypes(include='number')  # Select numeric columns\n",
    "\n",
    "# Iteratively remove columns with high correlation\n",
    "while True:\n",
    "    updated_websites = remove_highly_correlated_columns(websites_cleaned)\n",
    "    if updated_websites.equals(websites_cleaned):\n",
    "        break  # Stop if no columns are removed\n",
    "    websites_cleaned = updated_websites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the identified columns. After analizing the heat map, I decided to drop these columns\n",
    "\n",
    "websites.drop(['NUMBER_SPECIAL_CHARACTERS', 'SOURCE_APP_PACKETS', 'TCP_CONVERSATION_EXCHANGE'], axis=1, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(websites.shape)\n",
    "print(websites.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Check for missing values\n",
    "missing_values = websites.isnull().sum()\n",
    "print(\"Missing Values in Each Column:\\n\", missing_values)\n",
    "\n",
    "# Total number of missing values in the dataset\n",
    "total_missing = websites.isnull().sum().sum()\n",
    "print(\"\\nTotal Missing Values in the Dataset:\", total_missing)\n",
    "\n",
    "# Percentage of missing values per column\n",
    "missing_percentage = (websites.isnull().sum() / len(websites)) * 100\n",
    "print(\"\\nPercentage of Missing Values in Each Column:\\n\", missing_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firts we will drop the columns with more than 50% of missing data\n",
    "\n",
    "# No column with more than 50% Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will drop the rows with missing data\n",
    "\n",
    "# Check for missing values before dropping rows\n",
    "missing_values_before = websites.isnull().sum().sum()\n",
    "print(f\"Total missing values before dropping rows: {missing_values_before}\")\n",
    "\n",
    "# Drop rows with missing data\n",
    "websites = websites.dropna()\n",
    "\n",
    "# Check for missing values after dropping rows\n",
    "missing_values_after = websites.isnull().sum().sum()\n",
    "print(f\"Total missing values after dropping rows: {missing_values_after}\")\n",
    "\n",
    "# Confirm the updated dataset dimensions\n",
    "print(f\"Dataset shape after dropping rows: {websites.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine missing values in each column\n",
    "\n",
    "# Check for remaining missing values in each column\n",
    "missing_values = websites.isnull().sum()\n",
    "\n",
    "# Display missing values per column\n",
    "print(\"Missing Values in Each Column After Cleaning:\\n\", missing_values)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "websites.WHOIS_COUNTRY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "good_country = {'None':'None', \n",
    "                'US':'US', \n",
    "                'SC':'SC', \n",
    "                'GB':'UK', \n",
    "                'UK':'UK', \n",
    "                'RU':'RU', \n",
    "                'AU':'AU', \n",
    "                'CA':'CA',\n",
    "                'PA':'PA',\n",
    "                'se':'SE', \n",
    "                'IN':'IN',\n",
    "                'LU':'LU', \n",
    "                'TH':'TH', \n",
    "                \"[u'GB'; u'UK']\":'UK', \n",
    "                'FR':'FR',\n",
    "                'NL':'NL',\n",
    "                'UG':'UG', \n",
    "                'JP':'JP', \n",
    "                'CN':'CN', \n",
    "                'SE':'SE',\n",
    "                'SI':'SI', \n",
    "                'IL':'IL', \n",
    "                'ru':'RU', \n",
    "                'KY':'KY', \n",
    "                'AT':'AT', \n",
    "                'CZ':'CZ', \n",
    "                'PH':'PH', \n",
    "                'BE':'BE', \n",
    "                'NO':'NO', \n",
    "                'TR':'TR', \n",
    "                'LV':'LV',\n",
    "                'DE':'DE', \n",
    "                'ES':'ES', \n",
    "                'BR':'BR', \n",
    "                'us':'US', \n",
    "                'KR':'KR', \n",
    "                'HK':'HK', \n",
    "                'UA':'UA', \n",
    "                'CH':'CH', \n",
    "                'United Kingdom':'UK',\n",
    "                'BS':'BS', \n",
    "                'PK':'PK', \n",
    "                'IT':'IT', \n",
    "                'Cyprus':'CY', \n",
    "                'BY':'BY', \n",
    "                'AE':'AE', \n",
    "                'IE':'IE', \n",
    "                'UY':'UY', \n",
    "                'KG':'KG'}\n",
    "\n",
    "websites.WHOIS_COUNTRY = websites.WHOIS_COUNTRY.apply(lambda x : good_country[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.WHOIS_COUNTRY.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bar_plot(x,y):\n",
    "    plt.bar(x, y)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_bar_plot(websites.WHOIS_COUNTRY.unique(),websites.WHOIS_COUNTRY.value_counts());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Identify WHOIS columns\n",
    "whois_columns = [col for col in websites.columns if col.startswith('WHOIS_')]\n",
    "\n",
    "# Print the top 10 values for each WHOIS column\n",
    "for col in whois_columns:\n",
    "    top_10_values = websites[col].value_counts().nlargest(10).index\n",
    "    print(f\"\\nTop 10 values for {col}:\\n\", top_10_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep top 10 values and replace the rest with \"OTHER\"\n",
    "for col in whois_columns:\n",
    "    top_10_values = websites[col].value_counts().nlargest(10).index  # Get the top 10\n",
    "    websites[col] = websites[col].apply(lambda x: x if x in top_10_values else \"OTHER\")\n",
    "\n",
    "# Verify the updated WHOIS_* columns\n",
    "for col in whois_columns:\n",
    "    print(f\"\\nUpdated {col} unique values:\\n\", websites[col].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_drop = ['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']\n",
    "\n",
    "# Drop the columns\n",
    "websites = websites.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Verify the updated dataset\n",
    "print(websites.shape)\n",
    "print(\"\\nUpdated dataset columns:\\n\", websites.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Display column data types\n",
    "print(\"\\nData Types in the Dataset:\\n\", websites.dtypes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# List of columns to drop\n",
    "column_to_drop = ['URL']\n",
    "\n",
    "# Drop the columns\n",
    "websites = websites.drop(columns=column_to_drop, errors='ignore')\n",
    "\n",
    "# Verify the updated dataset\n",
    "print(websites.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Print the unique value counts of CHARSET\n",
    "print(\"\\nUnique Value Counts of CHARSET:\\n\", websites['CHARSET'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Print the unique value counts of CHARSET\n",
    "print(\"\\nUnique Value Counts of SERVER:\\n\", websites['SERVER'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count `SERVER` value counts here\n",
    "\n",
    "# Function to categorize SERVER values\n",
    "def categorize_server(server):\n",
    "    server = str(server)  # Ensure it's a string\n",
    "    if \"Apache\" in server:\n",
    "        return \"Apache\"\n",
    "    elif \"Microsoft\" in server:\n",
    "        return \"Microsoft\"\n",
    "    elif \"nginx\" in server:\n",
    "        return \"nginx\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Apply the function to the SERVER column\n",
    "websites['SERVER'] = websites['SERVER'].apply(categorize_server)\n",
    "\n",
    "# Verify the unique values\n",
    "print(\"\\nUpdated SERVER Unique Values:\\n\", websites['SERVER'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Convert categorical columns to dummy variables\n",
    "website_dummy = pd.get_dummies(websites, drop_first=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(websites.shape)\n",
    "print(website_dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, inspect `website_dummy` to make sure the data and types are intended - there shouldn't be any categorical columns at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Verify the new dataframe\n",
    "print(\"\\nTransformed Dataset (website_dummy) Preview:\\n\", website_dummy.head())\n",
    "\n",
    "# Check the new column count\n",
    "print(\"\\nNew dataset shape:\", website_dummy.shape)\n",
    "\n",
    "print(\"\\nFinal Columns in website_dummy:\\n\", website_dummy.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = website_dummy.drop(columns=['Type'])  \n",
    "y = website_dummy['Type']  \n",
    "\n",
    "\n",
    "# Split into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Train-Test Split Completed Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check unique values in y_train\n",
    "print(\"Unique values in y_train:\", np.unique(y_train))\n",
    "\n",
    "# Check unique values in y_test\n",
    "print(\"Unique values in y_test:\", np.unique(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Logistic Regression model initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Fit (train) the model using the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "print(\"Logistic Regression model has been trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Compute the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(f\"\\nAccuracy Score: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define class labels \n",
    "class_labels = [\"Class 0\", \"Class 1\", \"Class 2\"] \n",
    "\n",
    "# Plot the heatmap \n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix Heatmap\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Compute predicted probabilities\n",
    "y_prob_dt = dt_model.predict_proba(X_test)[:, 1]  # Get probability of class 1\n",
    "\n",
    "# Convert probabilities to binary predictions using a threshold of 0.5\n",
    "y_pred_threshold = (y_prob_dt >= 0.5).astype(int)\n",
    "\n",
    "# Compute confusion matrix and accuracy score\n",
    "conf_matrix_threshold = confusion_matrix(y_test, y_pred_threshold)\n",
    "accuracy_threshold = accuracy_score(y_test, y_pred_threshold)\n",
    "\n",
    "# Print results\n",
    "print(\"Confusion Matrix (Threshold = 0.5):\\n\", conf_matrix_threshold)\n",
    "print(f\"\\nAccuracy Score (Threshold = 0.5): {accuracy_threshold:.4f}\")\n",
    "\n",
    "# Plot confusion matrix heatmap\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(conf_matrix_threshold, annot=True, cmap=\"Purples\", xticklabels=[\"Class 0\", \"Class 1\"], \n",
    "            yticklabels=[\"Class 0\", \"Class 1\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix (Threshold = 0.5)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has close to 100% acuracy, even though there seems to be an imbalance of types in this validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get unique values in 'Type' column\n",
    "print(website_dummy['Type'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the Decision Tree model with max_depth=3\n",
    "dt_model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "# Train the model using the training data\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Decision Tree Classifier trained successfully with max_depth=3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute predicted probabilities\n",
    "y_prob_dt = dt_model.predict_proba(X_test)[:, 1]  # Get probability of class 1\n",
    "\n",
    "# Convert probabilities to binary predictions using a threshold of 0.5\n",
    "y_pred_threshold = (y_prob_dt >= 0.5).astype(int)\n",
    "\n",
    "# Compute confusion matrix and accuracy score\n",
    "conf_matrix_threshold = confusion_matrix(y_test, y_pred_threshold)\n",
    "accuracy_threshold = accuracy_score(y_test, y_pred_threshold)\n",
    "\n",
    "# Print results\n",
    "print(\"Confusion Matrix (Threshold = 0.5):\\n\", conf_matrix_threshold)\n",
    "print(f\"\\nAccuracy Score (Threshold = 0.5): {accuracy_threshold:.4f}\")\n",
    "\n",
    "# Plot confusion matrix heatmap\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(conf_matrix_threshold, annot=True, cmap=\"Purples\", xticklabels=[\"Class 0\", \"Class 1\"], \n",
    "            yticklabels=[\"Class 0\", \"Class 1\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix (Threshold = 0.5)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the Decision Tree model with max_depth=3\n",
    "dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "\n",
    "# Train the model using the training data\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Decision Tree Classifier trained successfully with max_depth=5\")\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute predicted probabilities\n",
    "y_prob_dt = dt_model.predict_proba(X_test)[:, 1]  # Get probability of class 1\n",
    "\n",
    "# Convert probabilities to binary predictions using a threshold of 0.5\n",
    "y_pred_threshold = (y_prob_dt >= 0.5).astype(int)\n",
    "\n",
    "# Compute confusion matrix and accuracy score\n",
    "conf_matrix_threshold = confusion_matrix(y_test, y_pred_threshold)\n",
    "accuracy_threshold = accuracy_score(y_test, y_pred_threshold)\n",
    "\n",
    "# Print results\n",
    "print(\"Confusion Matrix (Threshold = 0.5):\\n\", conf_matrix_threshold)\n",
    "print(f\"\\nAccuracy Score (Threshold = 0.5): {accuracy_threshold:.4f}\")\n",
    "\n",
    "# Plot confusion matrix heatmap\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(conf_matrix_threshold, annot=True, cmap=\"Purples\", xticklabels=[\"Class 0\", \"Class 1\"], \n",
    "            yticklabels=[\"Class 0\", \"Class 1\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix (Threshold = 0.5)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was no improvement in the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
